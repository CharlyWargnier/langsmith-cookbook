{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddb1a3b-eaf7-4755-8bfe-4d9178c7927a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluating Existing Runs\n",
    "\n",
    "This tutorial shows how to evaluate and tag runs after they've already been logged to a project. This is useful when:\n",
    "- You have a new evaluator or version of an evaluator and want to add the eval metrics to existing test projects\n",
    "- You want to use AI-assisted feedback within monitoring projects (non-test projects)\n",
    "- Your model isn't defined in python or typescript but you want to add evaluation metrics\n",
    "\n",
    "The typical steps are:\n",
    "\n",
    "1. Select the runs you wish to evaluate (see the [run filtering](https://docs.smith.langchain.com/tracing/use-cases/export-runs/local) docs for more information)\n",
    "2. Define the RunEvaluator\n",
    "3. Call the `client.evaluate_run` method, which runs the evaluation and logs the results as feedback.\n",
    "\n",
    "    - _alternatively, call `client.create_feedback` method directly, since evaluation results are logged as model feedback_\n",
    "    \n",
    "\n",
    "This is all you need to start logging eval feedback to an existing project.\n",
    "Below, we will review how to list the runs to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2dbea3-77d3-4d2a-bc6e-39e3688eadab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Select runs to evaluate\n",
    "\n",
    "We will consider two scenarios:\n",
    "\n",
    "    a. Evaluating a test project (a project created when calling the `run_on_dataset` function)\n",
    "    b. Evaluating runs in a monitoring or debug project (all other projects)\n",
    "    \n",
    "In either case, once you have the project name or ID, you can list the runs to evaluate or add feedback to by calling the `list_runs` method on the client.\n",
    "    \n",
    "#### a. Test Projects\n",
    "\n",
    "Each time you run call `run_on_dataset` to evaluate a model, a new \"test project\" is created containing the model's runs and the evaluator feedback. Each run contains the inputs and outputs to the component as well as a reference to the dataset example (row) it came from.\n",
    "\n",
    "\n",
    "The easiest way to find the project name or ID is in the web app. Navigating to \"Datasets & Testing\", selecting a dataset, and then copying one of the project names from the test runs table. Below is an example of the Dataset & Testing page, with all the datasets listed out. We will select the \"Chat Langchain Questions\" dataset.\n",
    "\n",
    "<img src=\"./img/datasets_and_testing.png\" alt=\"Datasets & Testing Page\" style=\"width:75%\">\n",
    "\n",
    "Once you've selected one of the datasets, a list of test projects will be displayed. You can copy the project name from the table directly.\n",
    "\n",
    "<img src=\"./img/test_projects_page.png\" alt=\"Test Projects\" style=\"width:75%\">\n",
    "\n",
    "Or if you navigate to the test page, you can copy the project name from the title or the ID from the url.\n",
    "\n",
    "<img src=\"./img/test_page.png\" alt=\"Test Page\" style=\"width:75%\">\n",
    "\n",
    "\n",
    "Then once you have the project name or ID, you can list the runs to evaluate by calling `list_runs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21249b59-ed95-49b3-b06d-bdecc6159bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Copy the project name or ID and paste it in the corresponding field below\n",
    "runs = client.list_runs(\n",
    "    project_name = \"fedb3000a5be453b881bf3c4aa22b5cb-RetrievalQA\",\n",
    "    # Or by ID\n",
    "    # project_id = \"0fc4f999-bdd3-4a7e-b2d7-bdf837d57cd9\",\n",
    "    execution_order = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83baa90c-27ee-4539-8774-44191bab7ba9",
   "metadata": {},
   "source": [
    "Since this is a test project, each run will have a reference to the dataset example, meaning you can apply a labeled evaluator such as the [cot_qa](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.qa.eval_chain.CotQAEvalChain.html#langchain.evaluation.qa.eval_chain.CotQAEvalChain) evaluator to these runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bf771-6da1-4a98-840f-8800f07a8c9e",
   "metadata": {},
   "source": [
    "#### b. Monitoring and Debug Projects\n",
    "\n",
    "For all other projects, you can find the project name or ID directly in the \"[Projects](https://smith.langchain.com/projects)\" page for your organization. \n",
    "\n",
    "When applying feedback to a monitoring project, you may want to filter by time or other criteria. For instance, to fetch today's traces in the `default` project, you \n",
    "can use the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9216ab8-4a0e-438b-80c5-50a2427df910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "midnight = datetime.datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "runs = client.list_runs(\n",
    "    project_name=\"default\",\n",
    "    execution_order=1,\n",
    "    start_time = midnight\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec884ee8-c4af-4bc5-b9e7-e87fa4c4f97f",
   "metadata": {},
   "source": [
    "For information on more advanced filtering techniques, check out the [run filtering docs](https://docs.smith.langchain.com/tracing/use-cases/export-runs/local).\n",
    "Since these projects are not typically associated with a dataset, you will have to use 'reference-free' evaluators [Criteria evaluator](https://docs.smith.langchain.com/evaluation/evaluator-implementations#no-labels-criteria-evaluation) to create evaluation feedback without reference labels.\n",
    "\n",
    "Once you've decided the runs you want to evaluate, it's time to define the evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48efb84a-c84a-4903-95a8-e6c0d1e47fa4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Define Evaluator\n",
    "\n",
    "You may already know what you want to test to ensure you application is functioning as expected. In that case, you can easily add that logic to a custom evaluator to get started.\n",
    "You can also configure one of LangChain's off-the-shelf evaluators to use to test for things like correctness, helpfulness, embedding or string distance, or other metrics.\n",
    "For more information on some of the existing open source evaluators, check out the [documentation](https://python.langchain.com/docs/guides/evaluation).\n",
    "\n",
    "### Custom Evaluator\n",
    "\n",
    "You can add automated/algorithmic feedback to existing runs using just the SDK in two steps:\n",
    "1. Subclassing the RunEvaluator class and implementing the evaluate_run method\n",
    "2. Calling the `evaluate_run` method directly on the client\n",
    "\n",
    "The `evaluate_run` method loads a reference example if present, applies the evaluator to the run and optional example, and then logs the feedback to LangSmith.\n",
    "Below, create a custom evaluator that checks for any digits in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae6f9459-51fa-468c-bc65-0b965f5ba628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from evaluate import load\n",
    "from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "class ContainsDigits(RunEvaluator):\n",
    "\n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        if run.outputs is None:\n",
    "            raise ValueError(\"Run outputs cannot be None\")\n",
    "        prediction = str(next(iter(run.outputs.values())))\n",
    "        contains_digits = any(c.isdigit() for c in prediction)\n",
    "        return EvaluationResult(key=\"Contains Digits\", score=contains_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb96fbe-ff1b-42d1-be98-ce4d2f312b39",
   "metadata": {},
   "source": [
    "Our custom evaluator is a simple reference-free check for boolean presence of digits in the output. In your case you may want to check for PII, assert the result conforms to some schema, or even parse and compare generated code.\n",
    "\n",
    "The logic fetching the prediction above assumes your chain only returns one value, meaning the `run.outputs` dictionary will have only one key. If there are multiple keys in your outputs, you will have to select whichever key(s) you wish to evaluate or test the whole outputs dictionary directly as a string. For more information on creating a custom evaluator, check out the [docs](https://docs.smith.langchain.com/evaluation/custom-evaluators).\n",
    "\n",
    "Below, apply the evaluator to all runs in the \"My Test\" project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df31b712-232e-4bfc-8298-4d6b656a02b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_name=\"LangSmith Retrieval QA Project\"\n",
    "evaluator = ContainsDigits()\n",
    "runs = client.list_runs(\n",
    "    project_name=project_name,\n",
    "    execution_order=1,\n",
    ")\n",
    "\n",
    "for run in runs:\n",
    "    feedback = client.evaluate_run(run, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf14eb40-dd2d-416b-889a-ef879c21a1b3",
   "metadata": {},
   "source": [
    "The evaluation results will all be saved as feedback to the run trace. LangSmith aggregates the feedback over the project for you asynchronously, so after some time you will be\n",
    "able to see the feedback results directly on the project stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "088e262f-6f82-4e0b-afee-e414cc9bd38b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Perplexity': {'n': 3, 'avg': 20.9166269302368, 'mode': 12.5060758590698},\n",
       " 'Contains Digits': {'n': 7, 'avg': 0.42857142857142855, 'mode': 0},\n",
       " 'sufficient_code': {'n': 7, 'avg': 0.5714285714285714, 'mode': 1},\n",
       " 'LangSmith Category': {'n': 21, 'avg': None, 'mode': None},\n",
       " 'COT Contextual Accuracy': {'n': 7, 'avg': 0.7142857142857143, 'mode': 1}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Updating the aggregate stats is async, but after some time, the \"Contains Digits\" feedback will be available\n",
    "client.read_project(project_name=project_name).feedback_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59c106-7a98-41ef-a769-78005b6ab6c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LangChain evaluators\n",
    "\n",
    "LangChain has a number of evaluators you can  use off-the-shelf or modify to suit your needs. An easy way to use these is to modify the code above and apply the evaluator directly to the run. For more information on available LangChain evaluators, check out the [open source documentation](https://python.langchain.com/docs/guides/evaluation).\n",
    "\n",
    "Below, we will demonstrate this by using the criteria evaluator, which instructs an LLM to check that the prediction against the described criteria. \n",
    "In this case, we will check that the responses contain both a python and typescript example, if needed, since LangSmith's SDK supports both languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a8e56-ad86-4e7e-809b-f86704f0cd2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import evaluation, callbacks\n",
    "\n",
    "class SufficientCodeEvaluator(RunEvaluator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        criteria_description=(\n",
    "            \"If the submission contains code, does it contain both a python and typescript example?\"\n",
    "            \" Y if no code is needed or if both languages are present, N if response is only in one language\"\n",
    "        )\n",
    "        self.evaluator = evaluation.load_evaluator(\"criteria\", \n",
    "                                      criteria={\n",
    "                                          \"sufficient_code\": criteria_description\n",
    "                                      })\n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        question = next(iter(run.inputs.values()))\n",
    "        prediction = str(next(iter(run.outputs.values())))\n",
    "        with callbacks.collect_runs() as cb:\n",
    "            result = self.evaluator.evaluate_strings(input=question, prediction=prediction)\n",
    "            run_id = cb.traced_runs[0].id\n",
    "        return EvaluationResult(key=\"sufficient_code\", evaluator_info={\"__run\": {\"run_id\": run_id}}, **result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538ba5a-3b5b-47b0-a082-1a27b7cda014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs = client.list_runs(\n",
    "    project_name=project_name,\n",
    "    execution_order=1,\n",
    ")\n",
    "evaluator = SufficientCodeEvaluator()\n",
    "for run in runs:\n",
    "    feedback = client.evaluate_run(run, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865676a1-0a20-492f-ae02-c29044cce9e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (advanced) Using a custom chain\n",
    "\n",
    "In LangSmith, evaluation results are feedback. The `Evaluator` abstractions help to orchestrate this, but you can also choose to save any computed values as feedback on the run.\n",
    "This is especially easy when using LangChain runnables. \n",
    "\n",
    "Let's make an example that tags each run's input using an LLM. You could apply this to a sample of production runs to further categorize them.\n",
    "\n",
    "In the example below, we will use LangChain's runnable lambda to conveniently batch calls, and use a runnable chain to perform the tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbdd724-5b1d-410c-bc0d-124800d197df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import chat_models, prompts, callbacks, schema\n",
    "\n",
    "chain = (\n",
    "    prompts.ChatPromptTemplate.from_template(\n",
    "    \"The following is a user question:\\n<question>\\n{question}</question>\\n\\n\"\n",
    "    \"Categorize it into 1 of the following categories:\\n\"\n",
    "    \"- API\\n- Tracing\\n- Evaluation\\n- Off-Topic\\n- Other\\n\\nCategory:\")\n",
    "    | chat_models.ChatOpenAI()\n",
    "    | schema.output_parser.StrOutputParser()\n",
    ")\n",
    "\n",
    "def evaluate_run(run: Run, config: dict):\n",
    "    # You can get the run ID using the collect_runs callback manager\n",
    "    with callbacks.collect_runs() as cb:\n",
    "        result = chain.invoke({\"question\": next(iter(run.inputs.values()))}, config)\n",
    "        feedback = client.create_feedback(\n",
    "            run.id,\n",
    "            key=\"LangSmith Category\",\n",
    "            value=result,\n",
    "            source_run_id=cb.traced_runs[0].id,\n",
    "            feedback_source_type=\"model\",\n",
    "        )\n",
    "    return feedback\n",
    "\n",
    "wrapped_function = schema.runnable.RunnableLambda(evaluate_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd061c7-039c-4799-8eeb-2acffc082545",
   "metadata": {},
   "source": [
    "Here, we are using the `collect_runs` callback handler to easily fetch the run ID from the evaluation run. By adding it as the `source_run_id`, the feedback will retain a link from the evaluated run to the source run so you can see why the tag was generated. Below, we will log feedback to all the traces in the specified project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb835f9c-d670-4393-b8d7-9c2b8199498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = client.list_runs(\n",
    "    project_name=project_name,\n",
    "    execution_order=1,\n",
    ")\n",
    "all_feedback = wrapped_function.batch([run for run in runs], return_errors=True)\n",
    "\n",
    "# Example of the first feedback example\n",
    "all_feedback[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66e144-141a-4d23-b7c3-3627d8ae6265",
   "metadata": {},
   "source": [
    "Check out the target project to see the feedback appear as the runs are tagged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d1323-fa00-4ba3-afb5-bb349c0a1f4b",
   "metadata": {},
   "source": [
    "### Evaluating the whole trace\n",
    "\n",
    "For all the examples above, we've been working with individual runs. By default, LangSmith will not return all the nested run spans in the trace. \n",
    "For some evaluations, however, you may want to consider information contained across different child runs. You can do this by setting the `load_child_runs` argument to `True` when calling `evaluate_run`.\n",
    "\n",
    "An example of when this is useful is if you want to evaluate an agent's trajectory of actions. Below, we will create an agent trajectory evaluator to do this. The main steps are:\n",
    "\n",
    "Within the evaluator:\n",
    "- Select the `tool` child runs to represent the agents actions\n",
    "- Use an LLM to grade the action choices based on the responses at each turn and the final answer\n",
    "\n",
    "And then for accessing the data:\n",
    "- Query the project for runs by name to select the agent executor\n",
    "- Specify `load_child_runs=True` to direct the client to load the other child runs in the trace before evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827bfcd-c95f-47cc-8a8d-7a603f18e542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import evaluation, callbacks, agents\n",
    "\n",
    "class AgentTrajectoryEvaluator(RunEvaluator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluator = evaluation.load_evaluator(\"trajectory\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def construct_trajectory(run: Run):\n",
    "        trajectory = []\n",
    "        for run in (run.child_runs or []):\n",
    "            if run.run_type == \"tool\":\n",
    "                action = agents.agent.AgentAction(tool=run.name, tool_input=run.inputs['input'], log='')\n",
    "                trajectory.append((action, run.outputs['output']))\n",
    "        return trajectory\n",
    "        \n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        if run.outputs is None:\n",
    "            return EvaluationResult(key=\"trajectory\", score=None)\n",
    "        question = next(iter(run.inputs.values()))\n",
    "        prediction = str(next(iter(run.outputs.values())))\n",
    "        trajectory = self.construct_trajectory(run)\n",
    "        with callbacks.collect_runs() as cb:\n",
    "            try:\n",
    "                result = self.evaluator.evaluate_agent_trajectory(input=question,\n",
    "                                                                  prediction=prediction,\n",
    "                                                                  agent_trajectory=trajectory)\n",
    "            except:\n",
    "                # If the evaluation fails, we can log a null score\n",
    "                return EvaluationResult(key=\"trajectory\", score=None)\n",
    "            run_id = cb.traced_runs[0].id\n",
    "        return EvaluationResult(key=\"trajectory\", evaluator_info={\"__run\": {\"run_id\": run_id}}, **result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be4f1e-aad1-47ae-bc37-1218f17d83c7",
   "metadata": {},
   "source": [
    "The `construct_trajectory` method extracts the tool runs from amongst the trace's child runs and then passes them to the evaluator.\n",
    "\n",
    "Below, we will select runs by the \"AgentExecutor\" name to be sure we are appropriately applying our evaluator, then we will\n",
    "evaluate, remembering to set `load_child_runs=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92667e-8d5a-4bb2-b7da-a9454cf07e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_name = \"My Project\"\n",
    "runs = client.list_runs(\n",
    "    project_name=project_name,\n",
    "    execution_order=1,\n",
    "    filter='eq(name, \"AgentExecutor\")',\n",
    ")\n",
    "\n",
    "evaluator = AgentTrajectoryEvaluator()\n",
    "for run in runs:\n",
    "    feedback = client.evaluate_run(run.id, evaluator, load_child_runs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb41ef-3892-4385-8830-c6decfbf8f5c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congrats! You've run evals on an existing project and logged feedback to the traces. Now, all the feedback results are aggregated on the project page and can be monitored through the monitoring dashboard.\n",
    "\n",
    "You can use the examples in this notebook to help evaluate, tag, and organize your traces as needed. If you have other related questions, feel free to create an issue in this repo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aacf44a-92e6-45c9-84ee-602c6f8a3a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

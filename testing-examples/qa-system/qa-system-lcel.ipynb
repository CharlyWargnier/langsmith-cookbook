{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33c67",
   "metadata": {},
   "source": [
    "# Checking Hallucinations in a Q&A System\n",
    "\n",
    "In this example, you will create a chain using LangChain's [Expression Language](https://python.langchain.com/docs/guides/expression_language/) to perform question answering (Q&A) over langsmith documentation and then evalute it using a dataset and criteria directed to check for hallucinations. When evaluating any system for which accuracy is of importance, collecting labeled datasets and using strong correctness metrics (mixed with human spot checking and oversight) is the gold standard.\n",
    "\n",
    "But it's a challenge to collect and maintain up-to-date, labeled datasets.\n",
    "\n",
    "In the absence of labels, you can quantify other metrics like:\n",
    "- Faithfulness - How faithful is the generated response to the retrieved documents?\n",
    "- Relevance - How relevant is the response to the original question?\n",
    "- Helpfulness - How helpful is the response in resolving the intent behind the question?\n",
    "\n",
    "This example shows one way to measure this using llm-assisted evals. The main steps are:\n",
    "\n",
    "1. Define the retrieval-augmented generation (RAG) question and answering (Q&A) system.\n",
    "2. Create a dataset of questions.\n",
    "3. Define evaluation config.\n",
    "4. Run evaluation in LangSmith\n",
    "\n",
    "**Note:** Separately evaluating the retriever itself (using standard retrieval metrics) can be helpful alongside whole-system evaluations. This guide will focus on measuring the llm response **conditioned on seeing the selected documents**. To maximize your system effectiveness, you likely will want to also evaluate and tune the retriever itself.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "We will be using [LangSmith](https://smith.langchain.com) and langchain. Please configure your API Key appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c788783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env LANGCHAIN_API_KEY=<YOUR_API_KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe780fbd",
   "metadata": {},
   "source": [
    "We will be using langchain, openai, and chromadb for this example. Upgrade to the latest versions of these libraries to make sure\n",
    "you have the requisite functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9e7425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install -U langchain > /dev/null\n",
    "# %pip install -U langsmith > /dev/null\n",
    "# %pip install openai > /dev/null\n",
    "# %pip install chromadb > /dev/null\n",
    "# %pip install lxml > /dev/null\n",
    "# %pip install html2text > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afac8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env OPENAI_API_KEY=<YOUR-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96733a",
   "metadata": {},
   "source": [
    "## 1. Define RAG Q&A System\n",
    "\n",
    "For our example, we will create a Q&A system over the LangSmith documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d4f4f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import RecursiveUrlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "api_loader = RecursiveUrlLoader(\"https://docs.smith.langchain.com\")\n",
    "text_splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "doc_transformer = Html2TextTransformer()\n",
    "\n",
    "raw_documents = api_loader.load()\n",
    "transformed = doc_transformer.transform_documents(raw_documents)\n",
    "documents = text_splitter.split_documents(transformed)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373432a5-fa4c-46a6-ae71-d906b9e5ebfa",
   "metadata": {},
   "source": [
    "We will construct our chatbot using the expression language below, it is simple and incorporates the following components:\n",
    "- Retriever\n",
    "- Prompt template\n",
    "- LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0534dda6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from uuid import uuid4\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "        \" questions from LangChain's documentation.\"\n",
    "        \" LangChain is a framework for building applications using large language models.\"\n",
    "        \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\"),\n",
    "        (\"system\", \"{context}\"),\n",
    "        (\"human\",\"{question}\")\n",
    "    ]\n",
    ").partial(time=str(datetime.now()))\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    } | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80ab6e",
   "metadata": {},
   "source": [
    "## 2. Create a Dataset of Examples\n",
    "\n",
    "We are going to hard-code a list of input questions and then use the traces to assemble the retrieved documents into a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbcd3690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"How might I query for all runs in a project?\",\n",
    "    \"What's a langsmith dataset?\",\n",
    "    \"How do I use a traceable decorator?\",\n",
    "    \"Can I trace my Llama V2 llm?\",\n",
    "    \"Why do I have to set environment variables?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56b3a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.callbacks import tracing_v2_enabled\n",
    "\n",
    "random_salt = str(uuid4().hex[0:8])\n",
    "project_name = f\"Retrieval QA Question Generation - {random_salt}\"\n",
    "with tracing_v2_enabled(project_name=project_name):\n",
    "    for q in questions:\n",
    "        chain.invoke({\"question\": q})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea01326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9897e02a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"520px\"\n",
       "            src=\"https://dev.smith.langchain.com/public/bf47c80e-a452-458c-969c-6991453b8407/r?zoom=50\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x28f584ad0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "# Here's an example run:\n",
    "first_run = next(iter(client.list_runs(project_name=project_name, execution_order=1)))\n",
    "shared_link = client.share_run(first_run.id)\n",
    "IFrame(shared_link, width='1000px', height='520px', zoom=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3b46e1",
   "metadata": {},
   "source": [
    "##### Extract Data from Runs\n",
    "\n",
    "In this case, it's easy to extract the pieces we want from the fully hydrated chat prompt template in the LLM runs.\n",
    "Below, read the LLM runs in the project and parse the messages from the structure.\n",
    "\n",
    "In this case, we are NOT going to add outputs to the dataset examples, since we have not verified that the responses are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdcc3fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.load.load import loads\n",
    "\n",
    "example_inputs = []\n",
    "for run in client.list_runs(project_name=project_name, run_type=\"llm\"):\n",
    "    # Deserialize the langchain objects\n",
    "    messages = loads(json.dumps(run.inputs['messages']))\n",
    "    context = messages[1].content\n",
    "    question = messages[-1].content\n",
    "    example_inputs.append({'context': context, 'question': question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdae462",
   "metadata": {},
   "source": [
    "#### Create the dataset\n",
    "\n",
    "We will use the `create_example` method on the client to create each example row. In this case, we will treat the context like a reference and then direct the criteria evaluator to consider the reference information in grading relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d759f806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = f\"Retrieval QA Questions - {random_salt}\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "\n",
    "for data in example_inputs:\n",
    "    client.create_example(inputs={\"question\": data['question']}, outputs={\"context\": data['context']}, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dddd61",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "We will use a labeled criteria evaluator for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dedaff3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "faithfulness_criterion = \"\"\"Is the submission faithful to and fully consistent\\\n",
    " with the reference supporting documents? It cannot contradict anything therein.\"\"\"\n",
    "hallucination_criterion = \"\"\"Is all information presented in submission\\\n",
    " conveyed grounded in the reference documents? If ANYTHING is inferred,\\\n",
    " presumed, or fully and explicitly stated in the reference docs, then the submission fails: respond 'N'.\"\"\"\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators = [\n",
    "        RunEvalConfig.LabeledCriteria(\n",
    "            criteria={\"faithfulness\": faithfulness_criterion}\n",
    "        ),\n",
    "        RunEvalConfig.LabeledCriteria(\n",
    "            criteria={\"hallucination\": hallucination_criterion}\n",
    "        ),\n",
    "    ],\n",
    "    # If you are fetching many, large documents, you may need\n",
    "    # a larger token window for the evaluator.\n",
    "    # Claude 2 can perform reasonably well.\n",
    "    # In general, it is not recommended to use eval LLMs less\n",
    "    # capable than claude 2 or gpt-4\n",
    "    # eval_llm=ChatAnthropic(model=\"claude-2\", temperature=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f30ce874",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '696acf01f9e44a399eec71dfd3afa652-RunnableSequence' at:\n",
      "https://dev.smith.langchain.com/projects/p/d8f42fc8-c577-42b1-87e3-3c1cdfb5e71a?eval=true\n"
     ]
    }
   ],
   "source": [
    "res = await client.arun_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=chain,\n",
    "    evaluation=eval_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fe5ebfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': {'n': 5, 'avg': 0.2, 'mode': 0},\n",
       " 'hallucination': {'n': 5, 'avg': 0.2, 'mode': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = client.read_project(project_name=res[\"project_name\"])\n",
    "project.feedback_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

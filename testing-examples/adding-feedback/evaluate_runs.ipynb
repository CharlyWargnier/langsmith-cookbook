{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddb1a3b-eaf7-4755-8bfe-4d9178c7927a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluating Existing Runs\n",
    "\n",
    "This tutorial shows how to evaluate and tag runs after they've already been logged. This is useful for some of the following scenarios:\n",
    "- You have a new evaluator or version of an evaluator and want to add the eval metrics on existing test projects\n",
    "- You want to use AI-assisted feedback within monitoring projects (non-test projects)\n",
    "\n",
    "The typical steps are:\n",
    "- Define the RunEvaluator\n",
    "- Select the runs you wish to evaluate (see the [run filtering](https://docs.smith.langchain.com/tracing/use-cases/export-runs/local) docs for more information)\n",
    "- Call the `evaluate_run` method, which runs the evaluation and logs the results as feedback.\n",
    "\n",
    "In general, any evaluation results can be logged as feedback by calling the `client.create_feedback` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48efb84a-c84a-4903-95a8-e6c0d1e47fa4",
   "metadata": {},
   "source": [
    "## Using just the SDK\n",
    "\n",
    "You can add automated/algorithmic feedback to existing runs using just the SDK. The LangSmith client has a helpful `evaluate_run` method to apply a run evaluator to a traced run and save the resulting feedback for the run trace. Let's make an example evaluator to check if the output contains any numeric digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae6f9459-51fa-468c-bc65-0b965f5ba628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from evaluate import load\n",
    "from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "class ContainsDigits(RunEvaluator):\n",
    "\n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        if run.outputs is None:\n",
    "            raise ValueError(\"Run outputs cannot be None\")\n",
    "        prediction = str(next(iter(run.outputs.values())))\n",
    "        contains_digits = any(c.isdigit() for c in prediction)\n",
    "        return EvaluationResult(key=\"Contains Digits\", score=contains_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb96fbe-ff1b-42d1-be98-ce4d2f312b39",
   "metadata": {},
   "source": [
    "Here we've defined a simple check that returns `True` if the prediction contains any digits, and `False` otherwise.\n",
    "\n",
    "The logic above assumes your chain only returns one value, meaning the `run.outputs` dictionary will have only one key. If there are multiple keys in your outputs, you will have to select whichever key(s) you wish to evaluate.\n",
    "\n",
    "In this case, the evaluator is \"reference-free\", meaning we never would use the \"example\" argument even if present. Evaluators that require reference labels can only be applied to runs that are associated with a dataset example.\n",
    "\n",
    "For more information on creating a custom evaluator, check out the [docs](https://docs.smith.langchain.com/evaluation/custom-evaluators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df31b712-232e-4bfc-8298-4d6b656a02b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# In this case, we are \n",
    "project_name=\"1680dedc34134584be61a59eb5c3f31e-RunnableSequence\"\n",
    "\n",
    "evaluator = ContainsDigits()\n",
    "runs = client.list_runs(\n",
    "    project_name=project_name,\n",
    "    execution_order=1,\n",
    ")\n",
    "\n",
    "for run in runs:\n",
    "    feedback = client.evaluate_run(run, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf14eb40-dd2d-416b-889a-ef879c21a1b3",
   "metadata": {},
   "source": [
    "The evaluation results will all be saved as feedback to the run trace.\n",
    "\n",
    "In our case, we only used the run itself (not the example object) to generate the evaluation results, but some evaluators may require reference labels. You can apply these when the run comes from a test project created when calling the `run_on_dataset` function.\n",
    "This makes sure to assign the correct 'reference_example_id' to each run so that it is linked to that example in the dataset. Then when the client calls `evaluate_run`, it loads the example and passes it to the evaluator so it knows the ground truth for a given data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "088e262f-6f82-4e0b-afee-e414cc9bd38b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Perplexity': {'n': 3, 'avg': 20.9166269302368, 'mode': 12.5060758590698},\n",
       " 'Contains Digits': {'n': 7, 'avg': 0.42857142857142855, 'mode': 0},\n",
       " 'COT Contextual Accuracy': {'n': 7, 'avg': 0.7142857142857143, 'mode': 1}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Updating the aggregate stats is async, but after some time, the \"Contains Digits\" feedback will be available\n",
    "client.read_project(project_name=project_name).feedback_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59c106-7a98-41ef-a769-78005b6ab6c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using a LangChain evaluator\n",
    "\n",
    "LangChain has a number of evaluators you can  use off-the-shelf or modify to suit your needs. An easy way to use these is to modify the code above and apply the evaluator directly to the run. For more information on available LangChain evaluators, check out the [open source documentation](https://python.langchain.com/docs/guides/evaluation).\n",
    "\n",
    "Below, we will demonstrate this by using the criteria evaluator to use an LLM to check that the responses contain both a python and typescript example, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb7a8e56-ad86-4e7e-809b-f86704f0cd2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import evaluation, callbacks\n",
    "\n",
    "class SufficientCodeEvaluator(RunEvaluator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        criteria_description=(\n",
    "            \"If the submission contains code, does it contain both a python and typescript example?\"\n",
    "            \" Y if no code is needed or if both languages are present, N if response is only in one language\"\n",
    "        )\n",
    "        self.evaluator = evaluation.load_evaluator(\"criteria\", \n",
    "                                      criteria={\n",
    "                                          \"sufficient_code\": criteria_description\n",
    "                                      })\n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        question = next(iter(run.inputs.values()))\n",
    "        prediction = str(next(iter(run.outputs.values())))\n",
    "        with callbacks.collect_runs() as cb:\n",
    "            result = self.evaluator.evaluate_strings(input=question, prediction=prediction)\n",
    "            run_id = cb.traced_runs[0].id\n",
    "        return EvaluationResult(key=\"sufficient_code\", evaluator_info={\"__run\": {\"run_id\": run_id}}, **result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9538ba5a-3b5b-47b0-a082-1a27b7cda014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs = client.list_runs(\n",
    "    project_name=project_name,\n",
    "    execution_order=1,\n",
    ")\n",
    "evaluator = SufficientCodeEvaluator()\n",
    "for run in runs:\n",
    "    feedback = client.evaluate_run(run, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d1323-fa00-4ba3-afb5-bb349c0a1f4b",
   "metadata": {},
   "source": [
    "## Evaluating the whole trace\n",
    "\n",
    "For some evaluations, you may want to consider information contained in multiple runs within a nested trace. You can do this by setting the `load_child_runs` argument to `True` when calling `evaluate_run` and then selecting the desired information from within the run tree.\n",
    "\n",
    "An example of this is if you want to evaluate the sequence of actions in an agent's trajectory. Below, we will create an agent trajectory evaluator to do this. In this example we will:\n",
    "\n",
    "- Select the tool child runs to represent the agents actions\n",
    "- Use an LLM to grade the action choices based on the responses at each turn and the final answer\n",
    "- Query the project for runs by name to select the agent executor\n",
    "- Specify `load_child_runs=True` to direct the client to load the other child runs in the trace before evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b827bfcd-c95f-47cc-8a8d-7a603f18e542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import evaluation, callbacks, agents\n",
    "\n",
    "class AgentTrajectoryEvaluator(RunEvaluator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluator = evaluation.load_evaluator(\"trajectory\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def construct_trajectory(run: Run):\n",
    "        trajectory = []\n",
    "        for run in (run.child_runs or []):\n",
    "            if run.run_type == \"tool\":\n",
    "                action = agents.agent.AgentAction(tool=run.name, tool_input=run.inputs['input'], log='')\n",
    "                trajectory.append((action, run.outputs['output']))\n",
    "        return trajectory\n",
    "        \n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        if run.outputs is None:\n",
    "            return EvaluationResult(key=\"trajectory\", score=None)\n",
    "        question = next(iter(run.inputs.values()))\n",
    "        prediction = str(next(iter(run.outputs.values())))\n",
    "        trajectory = self.construct_trajectory(run)\n",
    "        with callbacks.collect_runs() as cb:\n",
    "            try:\n",
    "                result = self.evaluator.evaluate_agent_trajectory(input=question,\n",
    "                                                                  prediction=prediction,\n",
    "                                                                  agent_trajectory=trajectory)\n",
    "            except:\n",
    "                # If the evaluation fails, we can log a null score\n",
    "                return EvaluationResult(key=\"trajectory\", score=None)\n",
    "            run_id = cb.traced_runs[0].id\n",
    "        return EvaluationResult(key=\"trajectory\", evaluator_info={\"__run\": {\"run_id\": run_id}}, **result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92667e-8d5a-4bb2-b7da-a9454cf07e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_name = \"7034821cd22f47368cfde810d98375b1-AgentExecutor\"\n",
    "runs = client.list_runs(\n",
    "    project_name=project_name,\n",
    "    execution_order=1,\n",
    "    filter='eq(name, \"AgentExecutor\")',\n",
    ")\n",
    "\n",
    "evaluator = AgentTrajectoryEvaluator()\n",
    "for run in runs:\n",
    "    feedback = client.evaluate_run(run.id, evaluator, load_child_runs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865676a1-0a20-492f-ae02-c29044cce9e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using a custom chain\n",
    "\n",
    "In LangSmith, evaluation results are feedback. If you have a custom function or chain you'd like to use to evaluate a run, you can directly log the output using the `create_feedback` method on the client. If the evaluator algorithm is traced, you can optionally add the `source_run_id` to the feedback to have it associated in the app.\n",
    "\n",
    "Let's make an LLM-powered example that tries to automatically tag the results based on the input content.\n",
    "We will use LangChain's runnable lambda to conveniently batch calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccbdd724-5b1d-410c-bc0d-124800d197df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('1cae2b52-82cf-41c8-9b67-9f2b8180b66c'), created_at=datetime.datetime(2023, 8, 30, 14, 59, 22, 964340), modified_at=datetime.datetime(2023, 8, 30, 14, 59, 22, 964345), run_id=UUID('6ebc3c3b-9ab6-4bd9-832f-6d567c4bbedf'), key='LangSmith Category', score=None, value='Other', comment=None, correction=None, feedback_source=FeedbackSourceBase(type='model', metadata={'__run': {'run_id': '07dd5f11-dc55-4470-8261-b590986624fd'}}))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import chat_models, prompts, callbacks, schema\n",
    "\n",
    "chain = (\n",
    "    prompts.ChatPromptTemplate.from_template(\n",
    "    \"The following is a user question:\\n<question>\\n{question}</question>\\n\\n\"\n",
    "    \"Categorize it into 1 of the following categories:\\n\"\n",
    "    \"- API\\n- Tracing\\n- Evaluation\\n- Off-Topic\\n- Other\\n\\nCategory:\")\n",
    "    | chat_models.ChatOpenAI()\n",
    "    | schema.output_parser.StrOutputParser()\n",
    ")\n",
    "\n",
    "def evaluate_run(run: Run):\n",
    "    # You can get the run ID using the collect_runs callback manager\n",
    "    with callbacks.collect_runs() as cb:\n",
    "        result = chain.invoke({\"question\": next(iter(run.inputs.values()))})\n",
    "        feedback = client.create_feedback(\n",
    "            run.id,\n",
    "            key=\"LangSmith Category\",\n",
    "            value=result,\n",
    "            source_run_id=cb.traced_runs[0].id,\n",
    "            feedback_source_type=\"model\",\n",
    "        )\n",
    "    return feedback\n",
    "\n",
    "wrapped_function = schema.runnable.RunnableLambda(evaluate_run)\n",
    "\n",
    "runs = client.list_runs(\n",
    "    project_name=project_name,\n",
    "    execution_order=1,\n",
    ")\n",
    "all_feedback = wrapped_function.batch([run for run in runs], return_errors=True)\n",
    "\n",
    "# Example of the first feedback example\n",
    "all_feedback[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb41ef-3892-4385-8830-c6decfbf8f5c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial shows how to evaluate existing runs using evaluators or any chain. This is useful for some of the following scenarios:\n",
    "- You have already run an model on the dataset and want to add evaluation results from a new evaluator for additional metrics\n",
    "- You want to run an evaluator or chain to generate feedback on runs that aren't within a test project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aacf44a-92e6-45c9-84ee-602c6f8a3a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

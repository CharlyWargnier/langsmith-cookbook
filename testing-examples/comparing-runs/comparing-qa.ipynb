{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33c67",
   "metadata": {},
   "source": [
    "# Comparing Q&A System Outputs\n",
    "\n",
    "While comparing models using aggregate metrics over datasets is often the fastest way to compare, you can sometimes miss out on the finer performance details of a system. While you can\n",
    "often get more input by A/B testing variants in production, one thing you can do is generate comparison metrics.\n",
    "\n",
    "LangSmith currently doesn't have a native evaluation flow for pairwise evaluation of systems over a dataset. The following guide will walk through how you can still do this\n",
    "in code while saving the traces to LangSmith for later inspection. We will share some setup code with the QA evaluation system in other recipes.\n",
    "\n",
    "The main steps are:\n",
    "\n",
    "1. Create a dataset of questions and answers.\n",
    "2. Define candidate chains.\n",
    "3. Generate predictions over the dataset.\n",
    "3. Evaluate the pairwise results.\n",
    "4. Summarize aggregate results.\n",
    "\n",
    "In this case, we will test the impact of chunk sizes on our results.\n",
    "\n",
    "TODO all the other narration.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial uses OpenAI for the model, ChromaDB to store documents, and LangChain to compose the chain. To make sure the tracing and evals are set up for [LangSmith](https://smith.langchain.com), please configure your API Key appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c788783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env LANGCHAIN_API_KEY=<YOUR_API_KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe780fbd",
   "metadata": {},
   "source": [
    "Install the required packages. `lxml` and `html2text` are used by the document loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9e7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U \"langchain[openai]\" > /dev/null\n",
    "# %pip install chromadb > /dev/null\n",
    "# %pip install lxml > /dev/null\n",
    "# %pip install html2text > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afac8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env OPENAI_API_KEY=<YOUR-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80ab6e",
   "metadata": {},
   "source": [
    "## 1. Create a Dataset\n",
    "\n",
    "For our example, we will be evaluating a Q&A system over the LangSmith documentation. In order to measure aggregate accuracy, we'll need to create a list of example question-answer paris. We've hard-coded some below to demonstrate the process. In general, you'll want a lot more (>100) pairs to get more meaningful results. Drawing from actual queries can be helpful to ensure better representation of the domain.\n",
    "\n",
    "Below, we have hard-coded some question-answer pairs to evaluate and use the client's `create_example` method to create each example row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e98a80-bf37-457e-b31d-952292e76c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have some hard-coded examples here.\n",
    "examples = [\n",
    "    (\"What is LangChain?\", \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\"),\n",
    "    (\"How might I query for all runs in a project?\", \"client.list_runs(project_name='my-project-name'), or in TypeScript, client.ListRuns({projectName: 'my-project-anme'})\"),\n",
    "    (\"What's a langsmith dataset?\", \"A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.\"),\n",
    "    (\"How do I use a traceable decorator?\", \"\"\"The traceable decorator is available in the langsmith python SDK. To use, configure your environment with your API key,\\\n",
    "import the required function, decorate your function, and then call the function. Below is an example:\n",
    "```python\n",
    "from langsmith.run_helpers import traceable\n",
    "@traceable(run_type=\"chain\") # or \"llm\", etc.\n",
    "def my_function(input_param):\n",
    "    # Function logic goes here\n",
    "    return output\n",
    "result = my_function(input_param)\n",
    "```\"\"\"),\n",
    "    (\"Can I trace my Llama V2 llm?\", \"So long as you are using one of LangChain's LLM implementations, all your calls can be traced\"),\n",
    "    (\"Why do I have to set environment variables?\", \"Environment variables can tell your LangChain application to perform tracing and contain the information necessary to authenticate to LangSmith.\"\n",
    "     \" While there are other ways to connect, environment variables tend to be the simplest way to configure your application.\"),\n",
    "    (\"How do I move my project between organizations?\", \"LangSmith doesn't directly support moving projects between organizations.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5edb7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbcd3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Retrieval QA Questions\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "for q, a in examples:\n",
    "    client.create_example(inputs={\"question\": q}, outputs={\"answer\": a}, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976437f",
   "metadata": {},
   "source": [
    "## 2. Define RAG Q&A System\n",
    "\n",
    "Our Q&A system uses a simple retriever and LLM response generator. To break that down further, the chain will be composed of:\n",
    "\n",
    "1. A [VectorStoreRetriever](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.base.VectorStoreRetriever.html#langchain.vectorstores.base.VectorStoreRetriever) to retrieve documents. This uses:\n",
    "   - An embedding model to vectorize documents and user queries for retrieval. In this case, the [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) model.\n",
    "   - A vectorstore, in this case we will use [Chroma](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html#langchain.vectorstores.chroma.Chroma)\n",
    "2. A response generator. This uses:\n",
    "   - A [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html#langchain.prompts.chat.ChatPromptTemplate) to combine the query and documents. \n",
    "   - An LLM, in this case, the 16k token context window version of `gpt-3.5-turbo` via [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html#langchain.chat_models.openai.ChatOpenAI).\n",
    "\n",
    "We will combine them using LangChain's [expression syntax](https://python.langchain.com/docs/guides/expression_language/cookbook).\n",
    "\n",
    "First, load the documents to populate the vectorstore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95fab721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import RecursiveUrlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "api_loader = RecursiveUrlLoader(\"https://docs.smith.langchain.com\")\n",
    "doc_transformer = Html2TextTransformer()\n",
    "raw_documents = api_loader.load()\n",
    "transformed = doc_transformer.transform_documents(raw_documents)\n",
    "\n",
    "def create_retriever(transformed_documents, text_splitter):\n",
    "    documents = text_splitter.split_documents(transformed_documents)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc4b79-4219-4446-a00c-beda55c2205a",
   "metadata": {},
   "source": [
    "With the documents prepared, create the vectorstore retriever. This is what will be used to provide context when generating a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "057f0841-dd9f-4f75-8ff5-dbdda73f84ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "retriever = create_retriever(transformed, text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35f884-bb93-427d-a0ad-3858c449a1ea",
   "metadata": {},
   "source": [
    "Next up, we'll define the response generator. This responds to the user by injecting the retrieved documents and the user query into a prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f266e9-e4de-42ad-b41e-99ace4dc5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def create_chain(retriever):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "                \" questions from LangSmith's documentation.\"\n",
    "                \" LangChain is a framework for building applications using large language models.\"\n",
    "                \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\"),\n",
    "                (\"system\", \"{context}\"),\n",
    "                (\"human\",\"{question}\")\n",
    "            ]\n",
    "        ).partial(time=str(datetime.now()))\n",
    "\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "    response_generator = (\n",
    "        prompt \n",
    "        | model \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    chain = (\n",
    "        # The runnable map here routes the original inputs to a context and a question dictionary to pass to the response generator\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "            \"question\": itemgetter(\"question\")\n",
    "        }\n",
    "        | response_generator\n",
    "    )\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1028d963-d84b-4759-a5ca-087b27065485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_1 = create_chain(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5765c938-c5f6-4ee4-be6b-90b7b341b683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will halve both the chunk size and overlap\n",
    "text_splitter_2 = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "retriever_2 = create_retriever(transformed, text_splitter_2)\n",
    "\n",
    "chain_2 = create_chain(retriever_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dddd61",
   "metadata": {},
   "source": [
    "## 3. Evaluate the Chain\n",
    "\n",
    "We will only be using the pairwise evaluator for this walkthrough. In practice, you will want to also use a correctness evaluator to measure aggregate correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcce6fa1-00a8-4fe8-b3ce-506ae630267a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "examples = list(client.list_examples(dataset_name=\"Retrieval QA Questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b7febcf-7942-453c-bdac-c04ee25e082a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.callbacks import LangChainTracer\n",
    "\n",
    "uid = uuid.uuid4().hex[:8]\n",
    "# Make predictions for the second model\n",
    "project_name = f\"chain-1-eval-{uid}\"\n",
    "configs = [{\"callbacks\": [LangChainTracer(example_id=example.id, project_name=project_name)]} for example in examples]\n",
    "results = chain_1.batch([example.inputs for example in examples], config=configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb87222f-3a72-4cd8-a351-fea5f90c23ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions for the second model\n",
    "project_name_2 = f\"chain-2-eval-{uid}\"\n",
    "# Same as above but use a different project name\n",
    "configs = [{\"callbacks\": [LangChainTracer(example_id=example.id, project_name=project_name_2)]} for example in examples]\n",
    "results_2 = chain_2.batch([example.inputs for example in examples], config=configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf7c70f3-9c8f-436d-85ca-5a398c3b1369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import logging\n",
    "\n",
    "def predict_preference(value: dict, eval_chain) -> list:\n",
    "    pred_a, pred_b, input_, answer = value[\"prediction\"], value[\"prediction_b\"], value[\"input\"], value[\"answer\"]\n",
    "    a, b = \"a\", \"b\"\n",
    "    # Flip a coin to average out persistent positional bias\n",
    "    if random.random() < 0.5:\n",
    "        pred_a, pred_b = pred_b, pred_a\n",
    "        a, b = \"b\", \"a\"\n",
    "    try:\n",
    "        eval_res = eval_chain.evaluate_string_pairs(\n",
    "            prediction=pred_a,\n",
    "            prediction_b=pred_b,\n",
    "            input=input_,\n",
    "            reference=answer\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.warning(e)\n",
    "        return None\n",
    "    result_map = {\n",
    "        \"A\": a,\n",
    "        \"B\": b\n",
    "    }\n",
    "    # None means no preference\n",
    "    return result_map.get(eval_res[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f65e8572-18a3-46b3-936a-38e566b77922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.evaluation import load_evaluator\n",
    "import functools\n",
    "\n",
    "pairwise_evaluator = load_evaluator(\"labeled_pairwise_string\")\n",
    "eval_func = functools.partial(predict_preference, eval_chain=pairwise_evaluator)\n",
    "runnable = RunnableLambda(eval_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16969ec1-d27c-4f8e-9624-8c3f25c51f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Invalid verdict: Final decision: [[A. Verdict must be one of 'A', 'B', or 'C'.\n",
      "WARNING:root:Invalid verdict: Response B is better as it sticks closely to the reference answer and does not introduce any additional parameters not mentioned in the reference.. Verdict must be one of 'A', 'B', or 'C'.\n"
     ]
    }
   ],
   "source": [
    "batch_inputs = [{\"prediction\": pred, \"prediction_b\": pred_b, \"input\": e.inputs['question'], \"answer\": e.outputs['answer']} for pred, pred_b, e in zip(results, results_2, examples)]\n",
    "values = runnable.batch(batch_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ec77a-67b3-48e8-9cde-c9022641f245",
   "metadata": {},
   "source": [
    "# 4. Add Feedback\n",
    "\n",
    "Now that we've made predictions, we can add the feedback to the runs to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b04cad4e-1de9-4c50-aeb6-64a5a5a2086d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _get_run(example_id, project_name):\n",
    "    return next(iter(client.list_runs(reference_example_id=example_id, project_name=project_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dab66e93-8f7a-4da7-abf3-e11890af7b34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add feedback\n",
    "for example, value in zip(examples, values):\n",
    "    if value is None:\n",
    "        continue\n",
    "    chain_1_run = _get_run(example.id, project_name)\n",
    "    chain_2_run = _get_run(example.id, project_name_2)\n",
    "    run_ids = [chain_1_run.id, chain_2_run.id]\n",
    "    score_map = {'a': chain_1_run.id, 'b': chain_2_run.id}\n",
    "    score_map = {'a': (1, 0), 'b': (0, 1)}\n",
    "    for run_id, score in zip(run_ids, score_map[value]):\n",
    "        other_run_id = run_ids[1 - run_ids.index(run_id)]\n",
    "        client.create_feedback(run_id,\n",
    "                               key=\"preference\",\n",
    "                               score=score,\n",
    "                               source_info={\"compared_to\": other_run_id}\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "471969c3-ddb9-4e35-9ed7-c491713e2131",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/datasets/16fb82a2-d986-4d86-ba54-ea6698446d65'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the projects by navigating to the Testing and Datasets page.\n",
    "ds = client.read_dataset(dataset_name=\"Retrieval QA Questions\")\n",
    "ds.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5133931-7c5c-4773-8e45-e45c7855a44f",
   "metadata": {},
   "source": [
    "You can review the results in the app to see preferred outputs.\n",
    "\n",
    "To measure statistical significance, you can check out other guides, such as the LangChain OSS guide [here](https://python.langchain.com/docs/guides/evaluation/examples/comparisons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ca922-1c92-44af-aefe-b78873a9032d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

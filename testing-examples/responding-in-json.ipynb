{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "571ab0b8",
   "metadata": {},
   "source": [
    "# Testing Responding in JSON\n",
    "\n",
    "This notebook will cover how to benchmark examples of chains that should respond in JSON. We will:\n",
    "\n",
    "1. Create a dataset of test examples\n",
    "2. Upload that dataset to LangSmith\n",
    "3. Create multiple chains\n",
    "4. Define some evaluation criteria\n",
    "5. Run some tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ab7b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14345901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Structured JSON Dataset\"\n",
    "\n",
    "# Storing inputs in a dataset lets us\n",
    "# run chains and LLMs over a shared set of examples.\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"Extracting structured JSON\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa190d87",
   "metadata": {},
   "source": [
    "## Create a dataset of test examples\n",
    "\n",
    "Let's create a dataset of examples. Let's pretend we want to extract structured information from unstructured input and we want to be structured in JSON format. Let's pretend we want to extract a person's name and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8964729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "examples = [\n",
    "    # Standard example\n",
    "    (\"Julie is 13\", json.dumps({\"name\": \"Julie\", \"age\": 13})),\n",
    "    # Example with name in lower case\n",
    "    (\"ben is 9\", json.dumps({\"name\": \"Ben\", \"age\": 9})),\n",
    "    # Example with age spelled out\n",
    "    (\"Sam is thirty four\", json.dumps({\"name\": \"Sam\", \"age\": 34})),\n",
    "    # Examples without ground truth\n",
    "    (\"Bob is 17\", ),\n",
    "    (\"Molly is 2\", ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd874dd5",
   "metadata": {},
   "source": [
    "## Upload dataset to LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40ff2757",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in examples:\n",
    "    # Each example must be unique and have inputs defined.\n",
    "    # Outputs are optional\n",
    "    if len(example) == 1:\n",
    "        client.create_example(\n",
    "            inputs={\"input\": example[0]},\n",
    "            outputs=None,\n",
    "            dataset_id=dataset.id,\n",
    "        )\n",
    "    elif len(example) == 2:\n",
    "        client.create_example(\n",
    "            inputs={\"input\": example[0]},\n",
    "            outputs={\"output\": example[1]},\n",
    "            dataset_id=dataset.id,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891a498",
   "metadata": {},
   "source": [
    "## Create Multiple Chains\n",
    "\n",
    "At this point, let's just try out OpenAI vs Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4c61dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic, ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "instructions = \"\"\"Convert any user messages into valid json. You should only respond with \n",
    "\n",
    "```json\n",
    "...\n",
    "```\n",
    "\n",
    "Do NOT include any words before or after.\n",
    "\n",
    "For each user input, you should extract the name and age of person in question. \\\n",
    "You should use the `name` and `age` to extract that information. \\\n",
    "Name should always be a properly capitalized name, Age should always be an integer.\n",
    "\n",
    "For example, for the input `Jim is 10` would get a response of:\n",
    "\n",
    "```json\n",
    "{\"name\": \"Jim\", \"age\": 10}}\n",
    "```\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=instructions),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a47931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_openai():\n",
    "    return LLMChain(\n",
    "        prompt=prompt, \n",
    "        llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "        output_parser=StrOutputParser()\n",
    "    )\n",
    "\n",
    "def create_anthropic():\n",
    "    return LLMChain(\n",
    "        prompt=prompt,\n",
    "        llm=ChatAnthropic(temperature=0, model=\"claude-2\"),\n",
    "        output_parser=StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfa444",
   "metadata": {},
   "source": [
    "## Define Custom Evaluation Criteria\n",
    "\n",
    "We can now define some custom evaluation criteria. Let's define a few!\n",
    "\n",
    "1. Whether after some parsing the expected output is exactly the same as expected\n",
    "2. Whether any words were returned before ```json\n",
    "3. Whether the json that was returned was valid json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "708efb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, Optional\n",
    "\n",
    "from langchain.evaluation import StringEvaluator\n",
    "\n",
    "\n",
    "class ParsedEquality(StringEvaluator):\n",
    "\n",
    "\n",
    "    @property\n",
    "    def requires_input(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def requires_reference(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def evaluation_name(self) -> str:\n",
    "        return \"parsed_equality\"\n",
    "\n",
    "    def _evaluate_strings(\n",
    "        self,\n",
    "        prediction: str,\n",
    "        input: Optional[str] = None,\n",
    "        reference: Optional[str] = None,\n",
    "        **kwargs: Any\n",
    "    ) -> dict:\n",
    "        parsed = prediction.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        result = json.loads(parsed)\n",
    "        return {\"score\": json.dumps(parsed) == reference}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b502e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, Optional\n",
    "\n",
    "from langchain.evaluation import StringEvaluator\n",
    "\n",
    "\n",
    "class IsVerbose(StringEvaluator):\n",
    "\n",
    "\n",
    "    @property\n",
    "    def requires_input(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def requires_reference(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def evaluation_name(self) -> str:\n",
    "        return \"is_verbose\"\n",
    "\n",
    "    def _evaluate_strings(\n",
    "        self,\n",
    "        prediction: str,\n",
    "        input: Optional[str] = None,\n",
    "        reference: Optional[str] = None,\n",
    "        **kwargs: Any\n",
    "    ) -> dict:\n",
    "        parsed = prediction.split(\"```json\")[0]\n",
    "        return {\"score\": len(parsed.strip()) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8e8bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, Optional\n",
    "\n",
    "from langchain.evaluation import StringEvaluator\n",
    "\n",
    "\n",
    "class IsValidJSON(StringEvaluator):\n",
    "\n",
    "\n",
    "    @property\n",
    "    def requires_input(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def requires_reference(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def evaluation_name(self) -> str:\n",
    "        return \"is_valid_json\"\n",
    "\n",
    "    def _evaluate_strings(\n",
    "        self,\n",
    "        prediction: str,\n",
    "        input: Optional[str] = None,\n",
    "        reference: Optional[str] = None,\n",
    "        **kwargs: Any\n",
    "    ) -> dict:\n",
    "        try:\n",
    "            parsed = prediction.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "            result = json.loads(parsed)\n",
    "            return {\"score\": 1}\n",
    "        except:\n",
    "            return {\"score\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51558b63",
   "metadata": {},
   "source": [
    "## Run evaluation\n",
    "\n",
    "Now we can run evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e201a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '2023-08-01-17-18-39-LLMChain' at:\n",
      "https://smith.langchain.com/projects/p/51cf0351-e754-40cf-b6d8-ec7ccaf5e594?eval=true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': '2023-08-01-17-18-39-LLMChain',\n",
       " 'results': {'e6c57ab7-a74e-4d8c-aa1e-95b3d7a8dd9c': [' ```json\\n{\"name\": \"Molly\", \"age\": 2}\\n```'],\n",
       "  '36b6eb46-e7a7-479c-9eb2-69647c8b970e': [' ```json\\n{\"name\": \"Bob\", \"age\": 17}\\n```'],\n",
       "  'f8bc4d86-ceb2-4acb-961b-df7eda182ec0': [' ```json\\n{\"name\": \"Sam\", \"age\": 34}\\n```'],\n",
       "  '035228dd-3180-4296-83d5-c486d35e092b': [' ```json\\n{\"name\": \"Ben\", \"age\": 9}\\n```'],\n",
       "  'e7d56cbe-0ac1-4928-a0f5-6d319319bb26': [' ```json\\n{\"name\": \"Julie\", \"age\": 13}\\n```']}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    custom_evaluators = [\n",
    "        IsVerbose(), \n",
    "        IsValidJSON(), \n",
    "        # ParsedEquality()\n",
    "    ],\n",
    ")\n",
    "run_on_dataset(\n",
    "    client,\n",
    "    \"Structured JSON Dataset\",\n",
    "    create_anthropic,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd3925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

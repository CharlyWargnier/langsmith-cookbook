{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "571ab0b8",
   "metadata": {},
   "source": [
    "# Testing Responding in JSON\n",
    "\n",
    "This notebook will cover how to benchmark examples of chains that should respond in JSON. We will:\n",
    "\n",
    "1. Create a dataset of test examples\n",
    "2. Upload that dataset to LangSmith\n",
    "3. Create multiple chains\n",
    "4. Define some evaluation criteria\n",
    "5. Run some tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1014c8-bfba-4245-b695-150fd296ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env LANGCHAIN_ENDOPINT=\"https://api.smith.langchain.com\"\n",
    "# %env LANGCHAIN_API_KEY=\"<your-api-key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c1007-d77a-4ff2-9804-f5340ea77c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02bea6d-415c-4856-895f-7717f9c01809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from langchain.evaluation import load_evaluator\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb8014d2-9595-45cc-883e-4ea5388994b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = load_evaluator(\"labeled_criteria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0daabfc2-590c-4074-8679-bbf4a3667819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'The criteria for evaluation is the helpfulness of the submission. The input is a simple greeting \"Hi\" and the submission is a response asking how they can help. This is a helpful and appropriate response to the input as it opens up for further communication and offers assistance. Therefore, the submission meets the criteria. The reference does not seem to be relevant in this context as it is in a different language and does not provide any additional context or information.',\n",
       " 'value': None,\n",
       " 'score': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.evaluate_strings(prediction=\"How can i help?\", reference=\"Come se si puoÌ€\", input=\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14345901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Structured JSON Dataset\"\n",
    "\n",
    "# # Storing inputs in a dataset lets us\n",
    "# # run chains and LLMs over a shared set of examples.\n",
    "# dataset = client.create_dataset(\n",
    "#     dataset_name=dataset_name, description=\"Extracting structured JSON\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fac947-c26c-4f16-b3c8-7ce273111cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/dair-iitd/CaRB/master/data/gold/dev.tsv\", sep=\"\\t\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa190d87",
   "metadata": {},
   "source": [
    "## Create a dataset of test examples\n",
    "\n",
    "Let's create a dataset of examples. Let's pretend we want to extract structured information from unstructured input and we want to be structured in JSON format. Let's pretend we want to extract a person's name and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964729d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "examples = [\n",
    "    # Standard example\n",
    "    (\"Julie is 13\", json.dumps({\"name\": \"Julie\", \"age\": 13})),\n",
    "    # Example with name in lower case\n",
    "    (\"ben is 9\", json.dumps({\"name\": \"Ben\", \"age\": 9})),\n",
    "    # Example with age spelled out\n",
    "    (\"Sam is thirty four\", json.dumps({\"name\": \"Sam\", \"age\": 34})),\n",
    "    # Examples without ground truth\n",
    "    (\"Bob is 17\", ),\n",
    "    (\"Molly is 2\", ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd874dd5",
   "metadata": {},
   "source": [
    "## Upload dataset to LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff2757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for example in examples:\n",
    "#     # Each example must be unique and have inputs defined.\n",
    "#     # Outputs are optional\n",
    "#     if len(example) == 1:\n",
    "#         client.create_example(\n",
    "#             inputs={\"input\": example[0]},\n",
    "#             outputs=None,\n",
    "#             dataset_id=dataset.id,\n",
    "#         )\n",
    "#     elif len(example) == 2:\n",
    "#         client.create_example(\n",
    "#             inputs={\"input\": example[0]},\n",
    "#             outputs={\"output\": example[1]},\n",
    "#             dataset_id=dataset.id,\n",
    "#         )\n",
    "#     else:\n",
    "#         raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891a498",
   "metadata": {},
   "source": [
    "## Create Multiple Chains\n",
    "\n",
    "At this point, let's just try out OpenAI vs Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c61dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic, ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "instructions = \"\"\"Convert any user messages into valid json. You should only respond with \n",
    "\n",
    "```json\n",
    "...q\n",
    "```\n",
    "\n",
    "Do NOT include any words before or after.\n",
    "\n",
    "For each user input, you should extract the name and age of person in question. \\\n",
    "You should use the `name` and `age` to extract that information. \\\n",
    "Name should always be a properly capitalized name, Age should always be an integer.\n",
    "\n",
    "For example, for the input `Jim is 10` would get a response of:\n",
    "\n",
    "```json\n",
    "{\"name\": \"Jim\", \"age\": 10}}\n",
    "```\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=instructions),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47931f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_openai():\n",
    "    return LLMChain(\n",
    "        prompt=prompt, \n",
    "        llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "        output_parser=StrOutputParser()\n",
    "    )\n",
    "\n",
    "def create_anthropic():\n",
    "    return LLMChain(\n",
    "        prompt=prompt,\n",
    "        llm=ChatAnthropic(temperature=0, model=\"claude-2\"),\n",
    "        output_parser=StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfa444",
   "metadata": {},
   "source": [
    "## Define Custom Evaluation Criteria\n",
    "\n",
    "We can now define some custom evaluation criteria. Let's define a few!\n",
    "\n",
    "1. Whether after some parsing the expected output is exactly the same as expected\n",
    "2. Whether any words were returned before ```json\n",
    "3. Whether the json that was returned was valid json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51558b63",
   "metadata": {},
   "source": [
    "## Run evaluation\n",
    "\n",
    "Now we can run evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201a46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluation=[\"json_validity\", \"json_equality\"],\n",
    ")\n",
    "run_on_dataset(\n",
    "    client,\n",
    "    \"Structured JSON Dataset\",\n",
    "    create_anthropic,\n",
    "    evaluation=evaluation_config,\n",
    "    concurrency_level=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd3925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

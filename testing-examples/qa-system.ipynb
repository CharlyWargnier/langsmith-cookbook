{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33c67",
   "metadata": {},
   "source": [
    "# Checking for Hallucinations in a Q&A System\n",
    "\n",
    "When evaluating any system for which accuracy is of importance, collecting labeled datasets and using strong correctness metrics (mixed with human spot checking and oversight) is the gold standard.\n",
    "\n",
    "But it's a challenge to collect and maintain up-to-date, labeled datasets.\n",
    "\n",
    "In the absence of labels, you can quantify other metrics like:\n",
    "- Faithfulness - How faithful is the generated response to the retrieved documents?\n",
    "- Relevance - How relevant is the response to the original question?\n",
    "- Helpfulness - How helpful is the response in resolving the intent behind the question?\n",
    "\n",
    "This example shows one way to measure this using llm-assisted evals. The main steps are:\n",
    "\n",
    "1. Create a dataset of questions.\n",
    "2. Define the retrieval-augmented generation (RAG) question and answering (Q&A) system.\n",
    "3. Define the evaluators, which will generate metrics over the dataset.\n",
    "4. Run evaluation in LangSmith.\n",
    "\n",
    "**Note:** Separately evaluating the retriever itself (using standard retrieval metrics) can be helpful alongside whole-system evaluations. This guide will focus on measuring the llm response **conditioned on seeing the selected documents**. To maximize your system effectiveness, you likely will want to also evaluate and tune the retriever itself.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "We will be using [LangSmith](https://smith.langchain.com) and langchain. Please configure your API Key appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c788783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env LANGCHAIN_API_KEY=<YOUR_API_KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe780fbd",
   "metadata": {},
   "source": [
    "We will also be using Chroma and OpenAI for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain > /dev/null\n",
    "# %pip install chromadb > /dev/null\n",
    "# %pip install lxml > /dev/null\n",
    "# %pip install html2text > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env OPENAI_API_KEY=<YOUR-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80ab6e",
   "metadata": {},
   "source": [
    "## 1. Create a Dataset of Examples\n",
    "\n",
    "We are going to hard-code a list of input questions to evaluate and the `create_example` method on the client to create each example row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5edb7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbcd3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"How might I query for all runs in a project?\",\n",
    "    \"What's a langsmith dataset?\",\n",
    "    \"How do I use a traceable decorator?\",\n",
    "    \"Can I trace my Llama V2 llm?\",\n",
    "    \"Why do I have to set environment variables?\"\n",
    "]\n",
    "\n",
    "dataset_name = \"Retrieval QA Questions\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "\n",
    "for q in questions:\n",
    "    client.create_example(inputs={\"question\": q}, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976437f",
   "metadata": {},
   "source": [
    "## 2. Define RAG Q&A System\n",
    "\n",
    "For our example, we will use a Q&A system over the LangSmith documentation. The chain will be composed of:\n",
    "\n",
    "1. An embedding model to vectorize documents and user queries for retrieval. In this case, the [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) model\n",
    "2. A [VectorStoreRetriever](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.base.VectorStoreRetriever.html#langchain.vectorstores.base.VectorStoreRetriever) to retrieve documents. We will use [Chroma](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html#langchain.vectorstores.chroma.Chroma) in this example.\n",
    "3. A [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html#langchain.prompts.chat.ChatPromptTemplate) to combine the query and documents. \n",
    "4. An LLM, in this case, gpt 3.5 turbo via [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html#langchain.chat_models.openai.ChatOpenAI).\n",
    "\n",
    "We will combine them using the [expression syntax](https://python.langchain.com/docs/guides/expression_language/cookbook).\n",
    "\n",
    "First, load the documents to populate the vectorstore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95fab721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import RecursiveUrlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "api_loader = RecursiveUrlLoader(\"https://docs.smith.langchain.com\")\n",
    "text_splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "doc_transformer = Html2TextTransformer()\n",
    "\n",
    "raw_documents = api_loader.load()\n",
    "transformed = doc_transformer.transform_documents(raw_documents)\n",
    "documents = text_splitter.split_documents(transformed)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73fb1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "\n",
    "def get_chain(retriever):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "            \" questions from LangChain's documentation.\"\n",
    "            \" LangChain is a framework for building applications using large language models.\"\n",
    "            \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\"),\n",
    "            (\"system\", \"{context}\"),\n",
    "            (\"human\",\"{question}\")\n",
    "        ]\n",
    "    ).partial(time=str(datetime.now()))\n",
    "    \n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "            \"question\": itemgetter(\"question\")\n",
    "        }\n",
    "        | prompt \n",
    "        | model \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "def eval_wrapper(example_input: dict) -> dict:\n",
    "    # Our production chain doesn't return the docs to the end user, but we want to surface them\n",
    "    # for our evaluator. We can wrap the retriever to pipe out the retrieved docs.\n",
    "    collected_docs = None\n",
    "    def collect_docs(docs: list) -> list:\n",
    "        \"\"\"Pass-through to return the retrieved docs as well.\"\"\"\n",
    "        nonlocal collected_docs\n",
    "        collected_docs = docs\n",
    "        return docs\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "    wrapped_retriever = retriever | collect_docs\n",
    "    chain = get_chain(wrapped_retriever)\n",
    "    result = chain.invoke(example_input)\n",
    "    return {\n",
    "        \"prediction\": result,\n",
    "        \"docs\": collected_docs\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dddd61",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "We will create a custom evaluator to apply to our retrieval Q&A model. In this case, we will wrap one of LangChain's criteria evaluators\n",
    "and treat the retrieved documents as a reference. This way we can evaluate the criterion against the grounded documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60848f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "\n",
    "class RetrievalEvaluator(RunEvaluator):\n",
    "    \"\"\"Evaluate the perplexity of a predicted string.\"\"\"\n",
    "\n",
    "    def __init__(self, criteria: dict):\n",
    "        self.criteria_evaluator = load_evaluator(\"labeled_criteria\", criteria=criteria)\n",
    "\n",
    "    def evaluate_run(self, run: Run, example: Example) -> EvaluationResult:\n",
    "        prediction, docs = run.outputs[\"prediction\"], run.outputs[\"docs\"]\n",
    "        docs_string = \"\\n\".join([doc['page_content'] for doc in docs])\n",
    "        res = self.criteria_evaluator.evaluate_strings(\n",
    "            prediction=prediction, \n",
    "            reference=docs_string,\n",
    "            input=run.inputs[\"question\"],\n",
    "            include_run_info=True,\n",
    "        )\n",
    "        return EvaluationResult(\n",
    "            key=self.criteria_evaluator.evaluation_name,\n",
    "            score=res.get(\"score\"),\n",
    "            comment=res.get(\"reasoning\"),\n",
    "            # Crosslink this run to the feedback\n",
    "            evaluator_info={\n",
    "                \"__run\": res[\"__run\"]\n",
    "            }\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dedaff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "faithfulness_criterion = \"\"\"Is the submission faithful to and fully consistent\\\n",
    " with the reference supporting documents? It cannot contradict anything therein.\"\"\"\n",
    "hallucination_criterion = \"\"\"Is all information presented in submission\\\n",
    " conveyed grounded in the reference documents? If ANYTHING is inferred,\\\n",
    " presumed, or fully and explicitly stated in the reference docs, then the submission fails: respond 'N'.\"\"\"\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    custom_evaluators = [\n",
    "        RetrievalEvaluator({\"faithfulness\": faithfulness_criterion}),\n",
    "        RetrievalEvaluator({\"hallucination\": hallucination_criterion}),\n",
    "    ],\n",
    "    # If you are fetching many, large documents, you may need\n",
    "    # a larger token window for the evaluator.\n",
    "    # Claude 2 can perform reasonably well.\n",
    "    # In general, it is not recommended to use eval LLMs less\n",
    "    # capable than claude 2 or gpt-4\n",
    "    # eval_llm=ChatAnthropic(model=\"claude-2\", temperature=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f30ce874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '3e9533de54124606843771d21a8584f4-RunnableLambda' at:\n",
      "https://dev.smith.langchain.com/projects/p/360f5072-f42f-4070-b600-485e5978d67c?eval=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-i0zjYONU3PemzJ222esBaAzZ on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n"
     ]
    }
   ],
   "source": [
    "_ = await client.arun_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=predict,\n",
    "    evaluation=eval_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4084596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33c67",
   "metadata": {},
   "source": [
    "# Checking for Hallucinations in a Q&A System\n",
    "\n",
    "When evaluating any system for which accuracy is of importance, collecting labeled datasets and using strong correctness metrics (mixed with human spot checking and oversight) is the gold standard.\n",
    "\n",
    "But it's a challenge to collect and maintain up-to-date, labeled datasets.\n",
    "\n",
    "In the absence of labels, you can quantify other metrics like:\n",
    "- Faithfulness - How faithful is the generated response to the retrieved documents?\n",
    "- Relevance - How relevant is the response to the original question?\n",
    "- Helpfulness - How helpful is the response in resolving the intent behind the question?\n",
    "\n",
    "This example shows one way to measure this using llm-assisted evals. The main steps are:\n",
    "\n",
    "1. Define the retrieval-augmented generation (RAG) question and answering (Q&A) system.\n",
    "2. Create a dataset of questions.\n",
    "3. Define evaluation config.\n",
    "4. Run evaluation in LangSmith\n",
    "\n",
    "**Note:** Separately evaluating the retriever itself (using standard retrieval metrics) can be helpful alongside whole-system evaluations. This guide will focus on measuring the llm response **conditioned on seeing the selected documents**. To maximize your system effectiveness, you likely will want to also evaluate and tune the retriever itself.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "We will be using [LangSmith](https://smith.langchain.com) and langchain. Please configure your API Key appropriately."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 1,
>>>>>>> a7646e7 (gs)
   "id": "4c788783",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# %env LANGCHAIN_API_KEY=<YOUR_API_KEY>"
=======
    "# %env LANGCHAIN_API_KEY=<YOUR-API-KEY>"
>>>>>>> a7646e7 (gs)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe780fbd",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "We will also be using Chroma and OpenAI for this example."
=======
    "We will also be using Chroma and OpenAI for this example"
>>>>>>> a7646e7 (gs)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 2,
>>>>>>> a7646e7 (gs)
   "id": "4f9e7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain > /dev/null\n",
    "# %pip install chromadb > /dev/null\n",
    "# %pip install lxml > /dev/null\n",
    "# %pip install html2text > /dev/null"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 3,
>>>>>>> a7646e7 (gs)
   "id": "afac8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env OPENAI_API_KEY=<YOUR-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96733a",
   "metadata": {},
   "source": [
    "## 1. Define RAG Q&A System\n",
    "\n",
    "For our example, we will use a Q&A system over the LangChain python documentation."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "id": "06d4f4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
=======
   "execution_count": 4,
   "id": "06d4f4f4",
   "metadata": {},
   "outputs": [],
>>>>>>> a7646e7 (gs)
   "source": [
    "from langchain.document_loaders import RecursiveUrlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
<<<<<<< HEAD
    "api_loader = RecursiveUrlLoader(\"https://docs.smith.langchain.com\")\n",
=======
    "api_loader = RecursiveUrlLoader(\"https://docs.smith.langchain.com\", crawl_siblings=True)\n",
>>>>>>> a7646e7 (gs)
    "text_splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "doc_transformer = Html2TextTransformer()\n",
    "\n",
    "raw_documents = api_loader.load()\n",
    "transformed = doc_transformer.transform_documents(raw_documents)\n",
    "documents = text_splitter.split_documents(transformed)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0534dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from uuid import uuid4\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "        \" questions from LangChain's documentation.\"\n",
    "        \" LangChain is a framework for building applications using large language models.\"\n",
    "        \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\"),\n",
    "        (\"system\", \"{context}\"),\n",
    "        (\"human\",\"{question}\")\n",
    "    ]\n",
    ").partial(time=str(datetime.now()))\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    } | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80ab6e",
   "metadata": {},
   "source": [
    "## 2. Create a Dataset of Examples\n",
    "\n",
    "We are going to hard-code a list of input questions and then use the traces to assemble the retrieved documents into a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbcd3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"How might I query for all runs in a project?\",\n",
    "    \"What's a langsmith dataset?\",\n",
    "    \"How do I use a traceable decorator?\",\n",
    "    \"Can I trace my Llama V2 llm?\",\n",
    "    \"Why do I have to set environment variables?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56b3a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import tracing_v2_enabled\n",
    "\n",
    "project_name = \"Retrieval QA Question Generation\"\n",
<<<<<<< HEAD
=======
    "from langsmith import Client; Client().delete_project(project_name=project_name)\n",
>>>>>>> a7646e7 (gs)
    "with tracing_v2_enabled(project_name=project_name):\n",
    "    for q in questions:\n",
    "        chain.invoke({\"question\": q})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea01326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9897e02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"520px\"\n",
<<<<<<< HEAD
       "            src=\"https://dev.smith.langchain.com/public/ebcf0666-387b-48f0-852a-fcac8b7a83d0/r?zoom=50\"\n",
=======
       "            src=\"https://dev.smith.langchain.com/public/0c956286-af04-460b-ba8a-59392acea6ca/r?zoom=50\"\n",
>>>>>>> a7646e7 (gs)
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
<<<<<<< HEAD
       "<IPython.lib.display.IFrame at 0x29a8e2f90>"
=======
       "<IPython.lib.display.IFrame at 0x175ff5650>"
>>>>>>> a7646e7 (gs)
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "# Here's an example run:\n",
    "first_run = next(iter(client.list_runs(project_name=project_name, execution_order=1)))\n",
    "shared_link = client.share_run(first_run.id)\n",
    "IFrame(shared_link, width='1000px', height='520px', zoom=50)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "3f3b46e1",
=======
   "id": "2806143f",
>>>>>>> a7646e7 (gs)
   "metadata": {},
   "source": [
    "##### Extract Data from Runs\n",
    "\n",
    "In this case, it's easy to extract the pieces we want from the fully hydrated chat prompt template in the LLM runs.\n",
    "Below, read the LLM runs in the project and parse the messages from the structure.\n",
    "\n",
    "In this case, we are NOT going to add outputs to the dataset examples, since we have not verified that the responses are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdcc3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.load.load import loads\n",
    "\n",
    "example_inputs = []\n",
    "for run in client.list_runs(project_name=project_name, run_type=\"llm\"):\n",
    "    # Deserialize the langchain objects\n",
    "    messages = loads(json.dumps(run.inputs['messages']))\n",
    "    context = messages[1].content\n",
    "    question = messages[-1].content\n",
    "    example_inputs.append({'context': context, 'question': question})"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "9bdae462",
=======
   "id": "9b341da5",
>>>>>>> a7646e7 (gs)
   "metadata": {},
   "source": [
    "#### Create the dataset\n",
    "\n",
    "We will use the `create_example` method on the client to create each example row. In this case, we will treat the context like a reference and then direct the criteria evaluator to consider the reference information in grading relevance."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "id": "d759f806",
=======
   "execution_count": 11,
   "id": "852e755a",
>>>>>>> a7646e7 (gs)
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Retrieval QA Questions\"\n",
<<<<<<< HEAD
=======
    "client.delete_dataset(dataset_name=dataset_name)\n",
>>>>>>> a7646e7 (gs)
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "\n",
    "for data in example_inputs:\n",
    "    client.create_example(inputs={\"question\": data['question']}, outputs={\"context\": data['context']}, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "f3dddd61",
=======
   "id": "03fc6b07",
>>>>>>> a7646e7 (gs)
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "We will use a labeled criteria evaluator for this"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "id": "dedaff3c",
=======
   "execution_count": 16,
   "id": "b2cd06c6",
>>>>>>> a7646e7 (gs)
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "faithfulness_criterion = \"\"\"Is the submission faithful to and fully consistent\\\n",
    " with the reference supporting documents? It cannot contradict anything therein.\"\"\"\n",
    "hallucination_criterion = \"\"\"Is all information presented in submission\\\n",
    " conveyed grounded in the reference documents? If ANYTHING is inferred,\\\n",
    " presumed, or fully and explicitly stated in the reference docs, then the submission fails: respond 'N'.\"\"\"\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators = [\n",
    "        RunEvalConfig.LabeledCriteria(\n",
    "            criteria={\"faithfulness\": faithfulness_criterion}\n",
    "        ),\n",
    "        RunEvalConfig.LabeledCriteria(\n",
    "            criteria={\"hallucination\": hallucination_criterion}\n",
    "        ),\n",
    "    ],\n",
    "    # If you are fetching many, large documents, you may need\n",
    "    # a larger token window for the evaluator.\n",
    "    # Claude 2 can perform reasonably well.\n",
    "    # In general, it is not recommended to use eval LLMs less\n",
    "    # capable than claude 2 or gpt-4\n",
    "    # eval_llm=ChatAnthropic(model=\"claude-2\", temperature=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "id": "f30ce874",
=======
   "execution_count": 17,
   "id": "f012e395",
>>>>>>> a7646e7 (gs)
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "View the evaluation results for project '4aa7d8c07e864f6b87e53675abefbd69-RunnableSequence' at:\n",
      "https://dev.smith.langchain.com/projects/p/ab3c63dd-5b27-41c8-8ba6-a5cd0778e377?eval=true\n"
=======
      "View the evaluation results for project '708b98da09ff4517ba97c561ffcd3ad8-RunnableSequence' at:\n",
      "https://dev.smith.langchain.com/projects/p/677de771-49bd-4dd6-8cf3-be1ca5958606?eval=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-i0zjYONU3PemzJ222esBaAzZ on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-i0zjYONU3PemzJ222esBaAzZ on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-i0zjYONU3PemzJ222esBaAzZ on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-i0zjYONU3PemzJ222esBaAzZ on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-i0zjYONU3PemzJ222esBaAzZ on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-i0zjYONU3PemzJ222esBaAzZ on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n"
>>>>>>> a7646e7 (gs)
     ]
    }
   ],
   "source": [
    "_ = await client.arun_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
<<<<<<< HEAD
    "    llm_or_chain_factory=chain,\n",
=======
    "    llm_or_chain_factory=lambda: chain,\n",
>>>>>>> a7646e7 (gs)
    "    evaluation=eval_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "3fe5ebfc",
=======
   "id": "3df9bc5d",
>>>>>>> a7646e7 (gs)
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

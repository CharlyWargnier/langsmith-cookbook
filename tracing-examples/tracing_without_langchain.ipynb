{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79036a8c",
   "metadata": {},
   "source": [
    "# Tracing Without LangChain\n",
    "\n",
    "LangSmith lets you instrument applications even if you don't want to depend on LangChain itself. The following is an example chat application using the raw openai SDK. First, install langsmith and the openai SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5615479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U pip > /dev/null\n",
    "%pip install -U langsmith > /dev/null\n",
    "%pip install -U openai > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebe2c0",
   "metadata": {},
   "source": [
    "Next, configure the API Key in the environment to make sure traces are logged to your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env LANGCHAIN_API_KEY=<your-api-key>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e6dfe",
   "metadata": {},
   "source": [
    "Next, define your chat application. Here we will use the `traceable` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba8c359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The @traceable decorator is experimental and will likely see breaking changes.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Tuple\n",
    "from langsmith.run_helpers import traceable\n",
    "\n",
    "_SYSTEM_PROMPT = f\"You are a helpful AI assistant. The current time is {datetime.now()}\"\n",
    "\n",
    "class Chat:\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\", api_key: Optional[str] = None, system_prompt: Optional[str] = _SYSTEM_PROMPT):\n",
    "        \"\"\"Initialize the Chat class with the specified model and set up memory.\n",
    "        \n",
    "        Args:\n",
    "            model (str): The ID of the model to use for the conversation.\n",
    "            api_key (str): The API key for authenticating with OpenAI.\n",
    "        \"\"\"\n",
    "        openai.api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n",
    "        \n",
    "        self.model = model\n",
    "        self.memory: List[Dict[str, str]] = []\n",
    "        self._system_prompt = system_prompt\n",
    "        self.reset_memory()\n",
    "            \n",
    "            \n",
    "    def _insert_system_prompt(self) -> None:\n",
    "        if self._system_prompt:\n",
    "            self.memory.insert(0, {\"role\": \"system\", \"content\": self._system_prompt})\n",
    "\n",
    "    @traceable(run_type=\"llm\")\n",
    "    def _call_openai(self, messages: List[List[dict]], model: str, temperature: float = 0.0):\n",
    "        return openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=messages[0],\n",
    "            stream=True,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    \n",
    "    @traceable(run_type=\"chain\")\n",
    "    def __call__(self, text: str) -> str:\n",
    "        \"\"\"Make a call to the OpenAI API to get a response for the provided text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text input from the user.\n",
    "        \n",
    "        Returns:\n",
    "            str: The model's response.\n",
    "        \"\"\"\n",
    "        self.memory.append({\"role\": \"user\", \"content\":  text})\n",
    "        response = self._call_openai([self.memory], self.model)\n",
    "        total_resp = \"\"\n",
    "        for chunk in response:\n",
    "            delta = chunk.choices[0].delta\n",
    "            if \"content\" in delta:\n",
    "                print(delta.content, end=\"\", flush=True)\n",
    "                total_resp += delta.content\n",
    "        self.memory.append({\"role\": \"assistant\", \"content\": total_resp})\n",
    "        print()\n",
    "        return total_resp\n",
    "    \n",
    "\n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear the chat memory.\"\"\"\n",
    "        self.memory = []\n",
    "        self._insert_system_prompt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a12a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "Hello Will! How can I assist you today?\n",
      "Alright, Will! If you have any questions or need assistance in the future, feel free to ask. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "chat = Chat()\n",
    "chat(\"Hi\")\n",
    "chat(\"My name as Will\")\n",
    "result = chat(\"No assisting needed actually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad8af8",
   "metadata": {},
   "source": [
    "## Capturing the runs\n",
    "\n",
    "The `traceable` decorator will inject the current `RunTree` object into the traced function if the argument is provided. You can use this to do things like fetch the run ID. Below, our `Chat2` app is an identical to the previous one but views the injected run_tree so we can fetch the latest run. The chat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117dca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import RunTree\n",
    "\n",
    "class Chat2(Chat):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.run_ids = []\n",
    "    \n",
    "    @traceable(run_type=\"chain\")\n",
    "    def __call__(self, text: str, run_tree: RunTree) -> str:\n",
    "        self.run_ids.append(run_tree.id)\n",
    "        return super().__call__(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56960b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b339dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "Hello, Will! How can I assist you today?\n",
      "Alright, Will! If you have any questions or need assistance in the future, feel free to reach out. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "result = chat(\"Hi\")\n",
    "chat(\"My name is Will\")\n",
    "result = chat(\"No assisting needed actually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c080ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_id = chat.run_ids[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e45a0845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"520px\"\n",
       "            src=\"https://dev.smith.langchain.com/public/922e9b80-19de-4c2c-bd50-ec9b02c2dc4f/r?zoom=50\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x13682f150>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "shared_link = client.share_run(latest_id)\n",
    "\n",
    "from IPython.display import IFrame\n",
    "# Here's an example run:\n",
    "IFrame(shared_link, width='1000px', height='520px', zoom=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1e02471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://dev.smith.langchain.com/public/922e9b80-19de-4c2c-bd50-ec9b02c2dc4f/r'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc2929",
   "metadata": {},
   "source": [
    "## Using the RunTree\n",
    "\n",
    "The `traceable` decorator is lightweight but less flexible when it comes to defining the schema of the logs you want to save. Below, we will make an example of the chat application that logs runs using the RunTree object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "561b3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Tuple\n",
    "from langsmith import RunTree\n",
    "\n",
    "_SYSTEM_PROMPT = f\"You are a helpful AI assistant. The current time is {datetime.now()}\"\n",
    "\n",
    "class ChatWithRunTree:\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\", api_key: Optional[str] = None, system_prompt: Optional[str] = _SYSTEM_PROMPT):\n",
    "        \"\"\"Initialize the Chat class with the specified model and set up memory.\n",
    "        \n",
    "        Args:\n",
    "            model (str): The ID of the model to use for the conversation.\n",
    "            api_key (str): The API key for authenticating with OpenAI.\n",
    "        \"\"\"\n",
    "        openai.api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n",
    "        \n",
    "        self.model = model\n",
    "        self.memory: List[Dict[str, str]] = []\n",
    "        self._system_prompt = system_prompt\n",
    "        self.reset_memory()\n",
    "        self.run_ids = []\n",
    "            \n",
    "            \n",
    "    def _insert_system_prompt(self) -> None:\n",
    "        if self._system_prompt:\n",
    "            self.memory.insert(0, {\"role\": \"system\", \"content\": self._system_prompt})\n",
    "\n",
    "    def _call_openai(self, messages: List[List[dict]], model: str, run_tree: RunTree = None, temperature: float = 0.0):\n",
    "        child = run_tree.create_child(\n",
    "            name=\"OpenAI.ChatCompletion\",\n",
    "            inputs={\n",
    "                \"messages\": messages,\n",
    "            },\n",
    "            extra={\n",
    "                \"temperature\": temperature,\n",
    "                \"model\": model\n",
    "            },\n",
    "            run_type=\"llm\",\n",
    "        )\n",
    "        child.post()\n",
    "        try:\n",
    "            resp = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=messages[0],\n",
    "                stream=True,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            child.end(error=str(e))\n",
    "            child.patch()\n",
    "            raise\n",
    "        child.end(outputs={\"response\": resp})\n",
    "        child.patch()\n",
    "        return resp\n",
    "    \n",
    "    def __call__(self, text: str) -> str:\n",
    "        \"\"\"Make a call to the OpenAI API to get a response for the provided text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text input from the user.\n",
    "        \n",
    "        Returns:\n",
    "            str: The model's response.\n",
    "        \"\"\"\n",
    "        tree = RunTree(name=\"Chat.__call__\", run_type=\"chain\", inputs={\"text\": text}, events=[])\n",
    "        tree.post()\n",
    "        try:\n",
    "            self.memory.append({\"role\": \"user\", \"content\":  text})\n",
    "            response = self._call_openai([self.memory], self.model, run_tree=tree)\n",
    "            total_resp = \"\"\n",
    "            for chunk in response:\n",
    "                delta = chunk.choices[0].delta\n",
    "                if \"content\" in delta:\n",
    "                    # You can add any dictionaries as run events\n",
    "                    tree.events.append(\n",
    "                        {\n",
    "                            \"name\": \"on_token\",\n",
    "                            \"content\": delta.content,\n",
    "                            \"time\": datetime.now()\n",
    "                        }\n",
    "                    )\n",
    "                    print(delta.content, end=\"\", flush=True)\n",
    "                    total_resp += delta.content\n",
    "            self.memory.append({\"role\": \"assistant\", \"content\": total_resp})\n",
    "        except Exception as e:\n",
    "            tree.end(error=str(e))\n",
    "            tree.patch()\n",
    "            raise e\n",
    "        tree.end(outputs={\"response\": total_resp})\n",
    "        tree.patch()\n",
    "        self.run_ids.append(tree.id)\n",
    "        print()\n",
    "        return total_resp\n",
    "    \n",
    "\n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear the chat memory.\"\"\"\n",
    "        self.memory = []\n",
    "        self._insert_system_prompt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7da0d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatWithRunTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9606938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "Hello Will! How can I assist you today?\n",
      "Alright, Will! If you have any questions or need assistance in the future, feel free to ask. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "chat = ChatWithRunTree()\n",
    "chat(\"Hi\")\n",
    "chat(\"My name as Will\")\n",
    "result = chat(\"No assisting needed actually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd165e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"520px\"\n",
       "            src=\"https://dev.smith.langchain.com/public/404b3828-c093-43e8-81ed-99cfd5d4840f/r?zoom=50\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x106564890>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_id = chat.run_ids[-1]\n",
    "shared_link = client.share_run(latest_id)\n",
    "IFrame(shared_link, width='1000px', height='520px', zoom=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728bc690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79036a8c",
   "metadata": {},
   "source": [
    "# Tracing Without LangChain\n",
    "\n",
    "LangSmith lets you instrument applications even if you don't want to depend on LangChain itself. This can help you debug, evaluate, and monotor any LLM application. In the following example, you will create an example series of LLM calls designed to generate, critique, and refine an argument. The whole series of calls will be traced to LangSmith, which will give you a run tree like the following:\n",
    "\n",
    "<img src=\"./img/snapshot_1.png\" alt=\"RunTree 1\">\n",
    "\n",
    "Before you get started, let's install some prerequisite packages. We'll need both the langsmith SDK and the OpenAI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5615479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U pip > /dev/null\n",
    "%pip install -U langsmith > /dev/null\n",
    "%pip install -U openai > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebe2c0",
   "metadata": {},
   "source": [
    "Next, configure the API Key in the environment to make sure traces are logged to your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f665dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env LANGCHAIN_API_KEY=<your-api-key>\n",
    "# %env LANGCHAIN_PROJECT=tracing-cookbook-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e6dfe",
   "metadata": {},
   "source": [
    "Next, define your chat application. Here we will use the `traceable` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba8c359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The @traceable decorator is experimental and will likely see breaking changes.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Tuple\n",
    "from langsmith.run_helpers import traceable\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_openai(data: List[dict], model: str = \"gpt-3.5-turbo\", temperature: float = 0.0):\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=data,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def argument_generator(query: str, additional_description: str = \"\") -> str:\n",
    "    return call_openai(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a debater making an argument on a topic.\"\n",
    "             f\"{additional_description}\"\n",
    "             f\" The current time is {datetime.now()}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"The discussion topic is {query}\"}\n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    "\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def critic(argument: str) -> str:\n",
    "    return call_openai(\n",
    "        [ \n",
    "            {\"role\": \"system\", \"content\": f\"You are a critic.\"\n",
    "           \"\\nWhat unresolved questions or criticism do you have after reading the following argument?\"\n",
    "            \"Provide a concise summary of your feedback.\"},\n",
    "            {\"role\": \"system\", \"content\": argument}\n",
    "            \n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    "\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def refiner(query: str, additional_description: str, current_arg: str, criticism: str) -> str:\n",
    "    return call_openai(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a debater making an argument on a topic.\"\n",
    "             f\"{additional_description}\"\n",
    "             f\" The current time is {datetime.now()}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"The discussion topic is {query}\"},\n",
    "            {\"role\": \"assistant\", \"content\": current_arg},\n",
    "            {\"role\": \"user\", \"content\": criticism},\n",
    "            {\"role\": \"system\", \"content\": \"Please generate a new argument that incorporates the feedback from the user.\"}\n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    "    \n",
    "\n",
    "@traceable(run_type=\"chain\")      \n",
    "def argument_chain(query: str, additional_description: str = \"\") -> str:\n",
    "    argument = argument_generator(query, additional_description)\n",
    "    criticism = critic(argument)\n",
    "    return refiner(query, additional_description, argument, criticism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f5798a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunshine, in moderation, is good for you because it provides essential vitamin D, which is crucial for bone health, immune function, and mental well-being. However, it is important to acknowledge the potential risks of excessive sun exposure, such as skin damage and an increased risk of skin cancer. By practicing sun safety measures, such as wearing sunscreen and seeking shade during peak hours, individuals can enjoy the benefits of sunshine while minimizing the potential drawbacks. Additionally, while there is anecdotal evidence suggesting that sunlight can improve mood and sleep patterns, further research is needed to fully understand and establish the causal relationship between sunlight and mental well-being.\n"
     ]
    }
   ],
   "source": [
    "result = argument_chain(\n",
    "    \"Whether sunshine is good for you.\", \n",
    "    additional_description=\"Provide a concise, few sentence argument on why sunshine is good for you.\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad8af8",
   "metadata": {},
   "source": [
    "## Capturing the runs\n",
    "\n",
    "The `traceable` decorator will inject the current `RunTree` object into the traced function if the argument is provided. You can use this to do things like fetch the run ID. Below, our `Chat2` app is an identical to the previous one but views the injected run_tree so we can fetch the latest run. The chat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117dca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import RunTree\n",
    "    \n",
    "run_ids = []\n",
    "    \n",
    "@traceable(run_type=\"chain\")      \n",
    "def argument_chain2(query: str, run_tree: RunTree,additional_description: str = \"\") -> str:\n",
    "    argument = argument_generator(query, additional_description)\n",
    "    run_ids.append(run_tree.id)\n",
    "    criticism = critic(argument)\n",
    "    return refiner(query, additional_description, argument, criticism)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b339dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = argument_chain2(\n",
    "    \"Whether sunshine is good for you.\", \n",
    "    additional_description=\"Provide a concise, few sentence argument on why sunshine is good for you.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c080ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_id = run_ids[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e45a0845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "shared_link = client.share_run(latest_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1656dd86",
   "metadata": {},
   "source": [
    "<img src=\"img/snapshot_2.png\" alt=\"RunTree 2\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1e02471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://dev.smith.langchain.com/public/6c28f481-044d-4be1-8ffe-c06e8773ea0a/r'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc2929",
   "metadata": {},
   "source": [
    "## Using the RunTree\n",
    "\n",
    "The `traceable` decorator is lightweight but less flexible when it comes to defining the schema of the logs you want to save. Below, we will make an example of the chat application that logs runs using the RunTree object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "561b3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai(data: List[dict], run_tree: RunTree,  model: str = \"gpt-3.5-turbo\", temperature: float = 0.0):\n",
    "    child = run_tree.create_child(\n",
    "        name=\"ChatOpenAI\",\n",
    "        inputs={\"data\": data, \"model\": model, \"temperature\": temperature},\n",
    "        run_type=\"llm\",\n",
    "    )\n",
    "    child.post()\n",
    "    try:\n",
    "        result = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=data,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        child.end(outputs={\"response\": result})\n",
    "    except Exception as e:\n",
    "        child.end(error=str(e))\n",
    "        raise\n",
    "    finally:\n",
    "        child.patch()\n",
    "    return result\n",
    "\n",
    "\n",
    "def argument_generator(query: str, run_tree: RunTree, additional_description: str = \"\") -> str:\n",
    "    child = run_tree.create_child(name=\"ArgumentGenerator\", inputs={\"query\": query, additional_description: additional_description}, run_type=\"chain\")\n",
    "    child.post()\n",
    "    try:\n",
    "        result = call_openai(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a debater making an argument on a topic.\"\n",
    "                    f\"{additional_description}\"\n",
    "                    f\" The current time is {datetime.now()}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"The discussion topic is {query}\"}\n",
    "            ], run_tree=child\n",
    "        ).choices[0].message.content\n",
    "        child.end(outputs={\"argument\": result})\n",
    "    except Exception as e:\n",
    "        child.end(error=str(e))\n",
    "        raise\n",
    "    finally:\n",
    "        child.patch()\n",
    "    return result\n",
    "\n",
    "\n",
    "def critic(argument: str, run_tree: RunTree) -> str:\n",
    "    child = run_tree.create_child(name=\"Critic\", run_type=\"chain\", inputs={\"argument\": argument})\n",
    "    child.post()\n",
    "    try:\n",
    "        result = call_openai(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a critic.\"\n",
    "                    \"\\nWhat unresolved questions or criticism do you have after reading the following argument?\"\n",
    "                    \"Provide a concise summary of your feedback.\"},\n",
    "                {\"role\": \"system\", \"content\": argument}\n",
    "            ], run_tree=child\n",
    "        ).choices[0].message.content\n",
    "        child.end(outputs={\"criticism\": result})\n",
    "    except Exception as e:\n",
    "        child.end(error=str(e))\n",
    "        raise\n",
    "    finally:\n",
    "        child.patch()\n",
    "    return result\n",
    "\n",
    "\n",
    "def refiner(query: str, additional_description: str, current_arg: str, criticism: str, run_tree: RunTree) -> str:\n",
    "    child = run_tree.create_child(name=\"Refiner\", run_type=\"chain\", inputs={\"query\": query, \"additional_description\": additional_description})\n",
    "    child.post()\n",
    "    try:\n",
    "        result = call_openai(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a debater making an argument on a topic.\"\n",
    "                    f\"{additional_description}\"\n",
    "                    f\" The current time is {datetime.now()}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"The discussion topic is {query}\"},\n",
    "                {\"role\": \"assistant\", \"content\": current_arg},\n",
    "                {\"role\": \"user\", \"content\": criticism},\n",
    "                {\"role\": \"system\", \"content\": \"Please generate a new argument that incorporates the feedback from the user.\"}\n",
    "            ], run_tree=child\n",
    "        ).choices[0].message.content\n",
    "        child.end(outputs={\"refined_argument\": result})\n",
    "    except Exception as e:\n",
    "        child.end(error=str(e))\n",
    "        raise\n",
    "    finally:\n",
    "        child.patch()\n",
    "    return result\n",
    "    \n",
    "\n",
    "run_tree_run_ids = []\n",
    "def argument_chain3(query: str, additional_description: str = \"\", run_tree: RunTree = None) -> str:\n",
    "    inputs = {\n",
    "        \"query\": query,\n",
    "        \"additional_description\": additional_description\n",
    "    }\n",
    "    if run_tree is None:\n",
    "        argument_run_tree = RunTree(name=\"ArgumentChain\", run_type=\"chain\", inputs=inputs)\n",
    "    else:\n",
    "        argument_run_tree = run_tree.create_child(name=\"ArgumentChain\", run_type=\"chain\", inputs=inputs)\n",
    "    argument_run_tree.post()\n",
    "    run_tree_run_ids.append(argument_run_tree.id)\n",
    "    try:\n",
    "        argument = argument_generator(query, additional_description=additional_description, run_tree=argument_run_tree)\n",
    "        criticism = critic(argument, run_tree=argument_run_tree)\n",
    "        result = refiner(query, additional_description, argument, criticism, run_tree=argument_run_tree)\n",
    "        argument_run_tree.end(outputs={\"final_argument\": result})\n",
    "    except Exception as e:\n",
    "        argument_run_tree.end(error=str(e))\n",
    "        raise\n",
    "    finally:\n",
    "        argument_run_tree.patch()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "728bc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = argument_chain3(\n",
    "    \"Whether sunshine is good for you.\", \n",
    "    additional_description=\"Provide a concise, few sentence argument on why sunshine is good for you.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d846394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://dev.smith.langchain.com/public/67d56aed-e25d-475a-9c56-4650c2f5394a/r'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_id = run_tree_run_ids[-1]\n",
    "shared_link = client.share_run(latest_id)\n",
    "shared_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d67f8b0",
   "metadata": {},
   "source": [
    "<img src=\"img/snapshot_3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a5a523",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

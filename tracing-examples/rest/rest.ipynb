{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ddd44f6-80a3-4c3c-b467-1a2a2cc05d62",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tracing via the REST API\n",
    "\n",
    "It's likely that your production LLM application is written in a language other than Python or JavaScript. In this case, you can use the REST API to log runs and take advantage of LangSmith's tracing and monitoring functionality. Doing so can be as simple as a POST request to the langsmith API. Specifically, to log runs you must:\n",
    "\n",
    "- Submit a POST request to \"https://api.smith.langchain.com/runs\"\n",
    "- JSON body of the request must have a name, run_type, inputs, and any other desired information.\n",
    "- An \"x-api-key\" header must be provided to authenticate, using a valid API key created in the LangSmith app\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Below is a minimal example of how to create a run using the REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5aff79e-8f03-4f3b-a347-032ce74bcd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "_LANGSMITH_API_KEY=os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "\n",
    "res = requests.post(\n",
    "    \"https://api.smith.langchain.com/runs\",\n",
    "    json={\n",
    "        \"name\": \"MyRun\",\n",
    "        \"run_type\": \"chain\",\n",
    "        \"inputs\": {\"text\": \"Foo\"},\n",
    "    },\n",
    "    headers={\n",
    "        \"x-api-key\": _LANGSMITH_API_KEY\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1521e322-8585-4d3a-acea-65eaa1a5334f",
   "metadata": {},
   "source": [
    "This will create a barebones \"chain\" run with the name \"MyRun\" and json inputs of {\"text\": \"Foo\"} that looks something like the following:\n",
    "\n",
    "<a href=\"https://smith.langchain.com/public/956e641a-4f1a-492d-9a67-3990f3fdba3e/r\"><img src=\"./img/minimal.png\" alt=\"minimal trace example\" style=\"width:75%\"></a>\n",
    "\n",
    "Not much information is included, since we haven't added outputs, tags, or metadata yet. It is also marked as \"pending\" since we haven't added an end time yet.\n",
    "\n",
    "In addition to the name, run_type, and inputs, you can also provide additional information. Below is an example of the supported schema:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"name\": \"string\",\n",
    "  \"inputs\": {},\n",
    "  \"run_type\": \"string\",\n",
    "  \"start_time\": \"2019-08-24T14:15:22Z\", # UTC timestamp in ISO format\n",
    "  \"end_time\": \"2019-08-24T14:15:22Z\", # UTC timestamp in ISO format\n",
    "  \"extra\": {},\n",
    "  \"error\": \"string\",\n",
    "  \"execution_order\": 1,\n",
    "  \"outputs\": {},\n",
    "  \"parent_run_id\": \"f8faf8c1-9778-49a4-9004-628cdb0047e5\",\n",
    "  \"events\": [\n",
    "    {}\n",
    "  ],\n",
    "  \"tags\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"id\": \"497f6eca-6276-4993-bfeb-53cbbbba6f08\",\n",
    "  \"session_id\": \"1ffd059c-17ea-40a8-8aef-70fd0307db82\",\n",
    "  \"session_name\": \"string\", # This is the name of the PROJECT. \"default\" if not specified. Sessions are the old name for projects.\n",
    "  \"reference_example_id\": \"9fb06aaa-105f-4c87-845f-47d62ffd7ee6\"\n",
    "}\n",
    "```\n",
    "\n",
    "This can also be found in the [API documentation](https://web.smith.langchain.com/redoc#tag/run/operation/create_run_runs_post).\n",
    "\n",
    "Lets look at a more complex chain example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b41f863-ba8e-41e5-908c-f74f770e00b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import datetime\n",
    "import platform\n",
    "\n",
    "start = datetime.datetime.utcnow() - datetime.timedelta(seconds=10)\n",
    "end = datetime.datetime.utcnow()\n",
    "res = requests.post(\n",
    "    \"https://api.smith.langchain.com/runs\",\n",
    "    json={\n",
    "        \"name\": \"ParentRun\",\n",
    "        \"run_type\": \"chain\",\n",
    "        \"inputs\": {\"text\": \"Foo\"},\n",
    "        \"outputs\": {\"generated\": \"Bar\"},\n",
    "        \"start_time\": start.isoformat(),\n",
    "        \"end_time\": end.isoformat(),\n",
    "        \"session_name\": \"My REST Project\",\n",
    "        \"tags\": [\"langsmith\", \"rest\", \"my-example\"],\n",
    "        \"events\": [\n",
    "            {\"event_name\": \"retry\", \"time\": start.isoformat()},\n",
    "            {\"event_name\": \"new_token\", \"value\": \"foo\"},\n",
    "        ],\n",
    "        \"extra\": {\n",
    "            \"metadata\": {\n",
    "                \"my_key\": \"My value\"\n",
    "            },\n",
    "            \"runtime\": {\n",
    "                \"platform\": platform.platform(),\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    headers={\n",
    "        \"x-api-key\": _LANGSMITH_API_KEY\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6cdc54-9604-47f5-886c-8c0dfc10ef1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Below is an example screenshot of what the logged trace above looks like. The new run now has inputs and outputs, a latency calculation, tags, and a status indicator.\n",
    "\n",
    "<a href=\"https://smith.langchain.com/public/69ca776a-60e4-4c4c-8b59-c8731e7f52f0/r\"><img src=\"./img/populated.png\" alt=\"populated trace example\" style=\"width:75%\"></a>\n",
    "\n",
    "To see the logged metadata and other runtime information you saved in the trace above, you can navigate to the \"metadata\" tab:\n",
    "    \n",
    "<a href=\"https://smith.langchain.com/public/69ca776a-60e4-4c4c-8b59-c8731e7f52f0/r?tab=2\"><img src=\"./img/populated_metadata.png\" alt=\"trace metadata\" style=\"width:75%\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9e644-bf1a-4b5d-802b-6eae3b55196c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logging LLM Runs\n",
    "\n",
    "The chain runs logged above are versatile and can be used to represent just about any function in your application. To get even more out of LangSmith, you'll want to also log your language model runs.\n",
    "\n",
    "Runs with the `run_type` of \"llm\" get some special treatment. They allow us to:\n",
    "\n",
    "- Help track token usage\n",
    "- Render \"prettier\" chat message formats for better readability.\n",
    "\n",
    "This is the case both for chat models and for regular \"completion\" LLMs.\n",
    "\n",
    "The easiest way to log LLM runs to LangSmith is by using OpenAI's llm message schema in the inputs and outputs.\n",
    "\n",
    "We will show examples below.\n",
    "\n",
    "#### Logging LLM Chat Messages\n",
    "\n",
    "To log messages in the \"chat\" model format (role and message dictionaries), LangSmith expects the following format:\n",
    "\n",
    "- Provide `messages: [{\"role\": string, \"content\": string}]` as a key-value pair in the inputs\n",
    "- Provide `choices: [{\"message\": {\"role\": string, \"content\": string}]` as a key-value pair in the outputs.\n",
    "\n",
    "For function calling, you can also pass a `functions=[...]` key-value pair in the inputs, and include a `function_call: {\"name\": string, \"arguments\": {}}` key-value pair in the message choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "167e66e6-e819-4c5d-b24e-d5f5236be86f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(\n",
    "    \"https://api.smith.langchain.com/runs\",\n",
    "    json={\n",
    "        \"name\": \"MyChatModelRun\",\n",
    "        \"run_type\": \"llm\",\n",
    "        \"inputs\": {\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in SF like?\"}],\n",
    "            # Optional\n",
    "            \"model\": \"text-davinci-003\", \n",
    "            \"functions\": [{\n",
    "              \"name\": \"get_current_weather\",\n",
    "              \"description\": \"Get the current weather in a given location\",\n",
    "              \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                  \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                  },\n",
    "                  \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                  }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "              }}],\n",
    "            # You can add other invocation paramers as k-v pairs\n",
    "            \"temperature\": 0.0,\n",
    "        },\n",
    "        \"outputs\": {\n",
    "              \"choices\": [\n",
    "                    {\n",
    "                      \"index\": 0,\n",
    "                      \"message\": {\n",
    "                        \"role\": \"assistant\",\n",
    "                        # Content is whatever string response the\n",
    "                        # model generates\n",
    "                        \"content\": \"Mostly cloudy.\",\n",
    "                        # Function call is the function invocation and arguments\n",
    "                        # as a string\n",
    "                        \"function_call\": {\n",
    "                          \"name\": \"get_current_weather\",\n",
    "                          \"arguments\": \"{\\n\\\"location\\\": \\\"San Francisco, CA\\\"\\n}\"\n",
    "                        }\n",
    "                      },\n",
    "                      \"finish_reason\": \"function_call\"\n",
    "                    }\n",
    "                  ],\n",
    "        },\n",
    "        \"start_time\": datetime.datetime.utcnow().isoformat(),\n",
    "        \"end_time\": datetime.datetime.utcnow().isoformat(),\n",
    "    },\n",
    "    headers={\n",
    "        \"x-api-key\": _LANGSMITH_API_KEY\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0a3663-bedf-4416-b565-720bfa56c300",
   "metadata": {},
   "source": [
    "When viewed in LangSmith, the run will look something like the one below, with the human, AI, and other chat messages all given their own cards, and with the token counts visible on the right.\n",
    "\n",
    "<a href=\"https://dev.smith.langchain.com/public/da2d7e1f-c8c7-4cd2-9e42-cbd7b74375bb/r\"><img src=\"./img/chat_example.png\" alt=\"chat example\" style=\"width:75%\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ca5d0-6beb-4882-b74b-e378b6839bf4",
   "metadata": {},
   "source": [
    "#### Logging \"Completions\" Models\n",
    "\n",
    "To log in the \"completions\" format (string in, string out), LangSmith expects the following format:\n",
    "- Name the run \"openai.Completion.create\" or \"openai.Completion.acreate\"\n",
    "- Provide `prompt: string` as a key-value pair in the inputs\n",
    "- Provide `choices: [{\"text\": string}]` key-value pair in the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67761b57-9c0c-441b-92b5-935b059bdaff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(\n",
    "    \"https://dev.api.smith.langchain.com/runs\",\n",
    "    json={\n",
    "        \"name\": \"MyLLMRun\",\n",
    "        \"run_type\": \"llm\",\n",
    "        \"inputs\": {\n",
    "            \"prompt\": \"Hi there!\",\n",
    "            # Optional: model or engine name, and other invocation params\n",
    "            \"engine\": \"text-davinci-003\",\n",
    "            \"temperature\": 0.0\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            'choices': [\n",
    "                {\n",
    "                   'text': '\\nMy name is Polly and I\\'m excited to talk to you!',\n",
    "                   'index': 0,\n",
    "                   'logprobs': None,\n",
    "                   'finish_reason': 'stop',\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "        \"end_time\": datetime.datetime.utcnow().isoformat(),\n",
    "        \"start_time\": datetime.datetime.utcnow().isoformat(),\n",
    "        \n",
    "    },\n",
    "    headers={\n",
    "        \"x-api-key\": os.environ[\"LANGCHAIN_DEV_API_KEY\"] # _LANGSMITH_API_KEY\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3b049-9192-4ff0-bb16-ecc527a91f7d",
   "metadata": {},
   "source": [
    "The completion model output looks like the example below. Once again, the token counts are indicated on the right, and the completion output is highlighted in green following the origina prompt.\n",
    "\n",
    "<a href=\"https://dev.smith.langchain.com/public/de23bfe5-d05e-46c6-aea1-664f159bd50c/r\"><img src=\"./img/completion_example.png\" alt=\"completion example\" style=\"width:75%\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492eb4ab-c5ef-457f-8733-caa0ea831e3a",
   "metadata": {},
   "source": [
    "## Updating Runs\n",
    "\n",
    "The examples above log runs in a single transaction. While simple, this leads to lower responsiveness in the web app and requires you to wait until after an operation has completed before posting any data.\n",
    "\n",
    "A more responsive approach is to post the inputs and other information when the run starts, then patch the outputs (or error message) when the run completes.\n",
    "\n",
    "The following example demonstrates this approach. We will first create post/patch functions to handle some of the boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40c75223-bf93-4393-a419-633a5e240a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Optional\n",
    "\n",
    "_LANGSMITH_API_KEY=os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "_LANGSMITH_PROJECT=os.environ.get(\"LANGCHAIN_PROJECT\", \"My REST Project\")\n",
    "\n",
    "def post_run(data: dict, name: str, run_id: str) -> None:\n",
    "    requests.post(\n",
    "        \"https://api.smith.langchain.com/runs\",\n",
    "        json={\n",
    "            \"id\": run_id,\n",
    "            \"name\": name,\n",
    "            \"run_type\": \"chain\",\n",
    "            \"inputs\": data,\n",
    "            \"start_time\": datetime.datetime.utcnow().isoformat(),\n",
    "            \"session_name\": _LANGSMITH_PROJECT,\n",
    "        },\n",
    "        headers={\n",
    "            \"x-api-key\": _LANGSMITH_API_KEY\n",
    "        }\n",
    "    )\n",
    "\n",
    "def patch_run(run_id, output: Optional[dict] = None, error: Optional[str] =None) -> None:\n",
    "    requests.patch(\n",
    "        f\"https://api.smith.langchain.com/runs/{run_id}\",\n",
    "        json={\n",
    "            \"error\": error,\n",
    "            \"outputs\": output,\n",
    "            \"end_time\": datetime.datetime.utcnow().isoformat(),\n",
    "        },\n",
    "        headers={\n",
    "            \"x-api-key\": _LANGSMITH_API_KEY\n",
    "        }\n",
    "    )     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8ca8f1-59a2-42c8-a046-db86193e2b8a",
   "metadata": {},
   "source": [
    "Now we will show how to use these functions in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0d337cf-e5b7-44e3-8cce-0d4c4420866e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_function(a, b):\n",
    "    run_id = str(uuid.uuid4())\n",
    "    post_run({\"a\": a, \"b\": b}, \"my_function\", run_id)\n",
    "    try:\n",
    "        result = a + b\n",
    "        patch_run(run_id, output={\"result\": result})\n",
    "    except Exception as e:\n",
    "        patch_run(run_id, error=str(e))\n",
    "        raise\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03b9d859-510b-4366-83a6-6ae590c3eda3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the function will always log to LangSmith now.\n",
    "my_function(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e2da84-0227-4c53-91d7-45cbbbc28a1e",
   "metadata": {},
   "source": [
    "Below is an example of the logged run. In our case since we are logging in the same thread as the main execution, there is a bit of added latency that is reflected in the run below.\n",
    "\n",
    "We typically recommend that you perform logging in a background thread.\n",
    "\n",
    "<a href=\"https://smith.langchain.com/public/22bcc8e4-208e-4fdc-ae11-ce5329263a88/r\"><img src=\"./img/streamed_result.png\" alt=\"streamed result\" style=\"width:75%\"></a>\n",
    "\n",
    "Now lets try logging with a value that will cause an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c808775-07e2-4a7b-af89-77ff034026d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This run will raise an error, with the error message logged to LangSmith\n",
    "try:\n",
    "    my_function(1, \"oops\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3afc5f-f2a0-4fb6-a727-0ad331cc26b7",
   "metadata": {},
   "source": [
    "As you can see in the screenshot below, the error has been succesfully logged, and no outputs were returned.\n",
    "\n",
    "<a href=\"https://smith.langchain.com/public/d33159dc-d8e7-493c-b5d8-4b6d83e1f401/r\"><img src=\"./img/streamed_error.png\" alt=\"streamed error\" style=\"width:75%\"></a>\n",
    "\n",
    "\n",
    "Before continuing, please be aware of the following requirements for run updates:\n",
    "- Once an end time has been assigned to a run, it is marked as finished and cannot be updated. \n",
    "- Only certain fields can be updated through the patch call (end_time, error, outputs, and events). Other fields can only be set in the initial POST call.\n",
    "\n",
    "For more information, see the [tracing FAQ](https://docs.smith.langchain.com/tracing/tracing-faq#when-logging-with-the-sdk-which-fields-can-i-update-when-i-patch) in the LangSmith documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd688a7-e4c9-450e-943e-70f8f1ed45ca",
   "metadata": {},
   "source": [
    "## Nesting Runs\n",
    "\n",
    "The above examples work great for linear logs, but it's likely that your application involves some amount of nested execution. It's a lot easier to debug a complex chain if the logs themselves contain the required associations. There are currently two bits of complexity in doing so. We plan to relax the execution order requirement at some point in the future:\n",
    "\n",
    "- You must include a `parent_run_id` in your JSON body.\n",
    "- You must track the `execution_order` of the child run for it to be rendered correctly in the trace.\n",
    "\n",
    "Below is an updated example of how to do this. We will create a new `RunLogger` class that manages the execution order state for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04e3cb65-94be-404a-8bab-45c7bd573052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Optional\n",
    "\n",
    "_LANGSMITH_API_KEY=os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "_LANGSMITH_PROJECT=os.environ.get(\"LANGCHAIN_PROJECT\", \"My REST Project\")\n",
    "\n",
    "class RunLogger:\n",
    "    def __init__(self):\n",
    "        self._run_map = {}\n",
    "        self._parents = {}\n",
    "        \n",
    "    def _get_execution_order(self, run_id: str, parent_run_id: Optional[str] = None) -> int:\n",
    "        if parent_run_id:\n",
    "            self._parents[run_id] = parent_run_id\n",
    "            execution_order = self._run_map.get(parent_run_id, 0)\n",
    "            execution_order += 1\n",
    "            self._run_map[parent_run_id] = execution_order\n",
    "        else:\n",
    "            execution_order = 1\n",
    "        self._run_map[run_id] = execution_order\n",
    "        return execution_order\n",
    "\n",
    "    def _update_execution_order(self, run_id: str) -> None:\n",
    "        exec_order = self._run_map[run_id]\n",
    "        parent_run_id = self._parents.pop(run_id, None)\n",
    "        if parent_run_id:\n",
    "            self._run_map[parent_run_id] = max(exec_order, self._run_map[parent_run_id])\n",
    "        \n",
    "    def post_run(self, data: dict, name: str, run_id: str, parent_run_id: Optional[str] = None) -> None:\n",
    "        execution_order = self._get_execution_order(run_id, parent_run_id)\n",
    "        requests.post(\n",
    "            \"https://api.smith.langchain.com/runs\",\n",
    "            json={\n",
    "                \"id\": run_id,\n",
    "                \"name\": name,\n",
    "                \"run_type\": \"chain\",\n",
    "                \"parent_run_id\": parent_run_id,\n",
    "                \"execution_order\": execution_order,\n",
    "                \"inputs\": data,\n",
    "                \"start_time\": datetime.datetime.utcnow().isoformat(),\n",
    "                \"session_name\": _LANGSMITH_PROJECT,\n",
    "            },\n",
    "            headers={\n",
    "                \"x-api-key\": _LANGSMITH_API_KEY\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def patch_run(self, run_id: str, output: Optional[dict] = None, error: Optional[str] =None) -> None:\n",
    "        self._update_execution_order(run_id)\n",
    "        requests.patch(\n",
    "            f\"https://api.smith.langchain.com/runs/{run_id}\",\n",
    "            json={\n",
    "                \"error\": error,\n",
    "                \"outputs\": output,\n",
    "                \"end_time\": datetime.datetime.utcnow().isoformat(),\n",
    "            },\n",
    "            headers={\n",
    "                \"x-api-key\": _LANGSMITH_API_KEY\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada92cd-470f-4b79-a923-7da895147c88",
   "metadata": {},
   "source": [
    "To demonstrate how this works, we will create a simple fibonacci function and log each call as a \"chain\" run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72331136-fca1-479c-81fc-ec98c9d9b1bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = RunLogger()\n",
    "\n",
    "def fibonacci(n: int, depth: int = 0, parent_run_id: Optional[str] = None) -> int:\n",
    "    run_id = str(uuid.uuid4())\n",
    "    logger.post_run({\"n\": n}, f\"fibonacci_recursive\", run_id, parent_run_id=parent_run_id)\n",
    "    try:\n",
    "        if n <= 1:\n",
    "            result = n\n",
    "        else:\n",
    "            result = fibonacci(n - 1, depth + 1, parent_run_id=run_id) + fibonacci(n - 2, depth + 1, parent_run_id=run_id)\n",
    "        logger.patch_run(run_id, output={\"result\": result})\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.patch_run(run_id, error=str(e))\n",
    "        raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e087cb60-1e12-48d5-8788-7cdab59993db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fibonacci(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973bdee1-78a5-4eb8-abce-4ad7b9880b00",
   "metadata": {
    "tags": []
   },
   "source": [
    "This should generate a trace similar to the one shown below:\n",
    "    \n",
    "<a href=\"https://smith.langchain.com/public/bd5f59eb-0b42-445c-a57f-352dc985eba4/r\"><img src=\"./img/fibonacci.png\" alt=\"fibonacci\" style=\"width:75%\"></a>\n",
    "\n",
    "All the calls are logged in their correct order.\n",
    "\n",
    "Similar to before, any error will be logged to LangSmith so you easily see where in the execution the chain failed. Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c713dff-7df2-4822-895a-1fd872bfc471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fibonacci(n: int, depth: int = 0, parent_run_id: Optional[str] = None) -> int:\n",
    "    run_id = str(uuid.uuid4())\n",
    "    logger.post_run({\"n\": n}, f\"fibonacci_recursive\", run_id, parent_run_id=parent_run_id)\n",
    "    try:\n",
    "        if n < 0:\n",
    "            raise ValueError(\"NEGATIVE NUMER NOT ALLOWED\")\n",
    "        if n <= 1:\n",
    "            result = n\n",
    "        else:\n",
    "            result = fibonacci(n - 1, depth + 1, parent_run_id=run_id) + fibonacci(n - 2, depth + 1, parent_run_id=run_id)\n",
    "        logger.patch_run(run_id, output={\"result\": result})\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.patch_run(run_id, error=str(e))\n",
    "        raise \n",
    "        \n",
    "# We will show what the trace looks like with an error\n",
    "try:\n",
    "    fibonacci(2.3)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada1f06-6222-4f44-9707-7ed41a95131e",
   "metadata": {
    "tags": []
   },
   "source": [
    "The resulting run should look something like the following. THe errors are propagated up the call hierarchy so you can easily see where in the execution the chain failed.\n",
    "\n",
    "<a href=\"https://smith.langchain.com/public/a3065970-1da6-472e-862b-eb9d97fd9607/r\"><img src=\"./img/recursive_error.png\" alt=\"fibonacci with error\" style=\"width:75%\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3dbd9-9fbb-403a-8f38-8e553bafe654",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this walkthrough, you used the REST API to log chain and LLM runs to LangSmith and reviewed the resulting traces. This is currently the only way to log runs to LangSmith if you aren't using a language supported by one of the LangSmith SDK's (python and JS/TS). You then used post/patch requests so the results would be usable more quickly and then logged nested runs to take advantage of LangSmith's full trace tree debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8458d9ab-4903-466e-accc-750136fe33b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

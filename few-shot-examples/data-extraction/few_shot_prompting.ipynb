{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08787e1-142d-4f63-be4f-f74e2c832da6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Few-Shot Prompting for Data Extraction\n",
    "\n",
    "Roadmap:\n",
    "\n",
    "    - Upload eval dataset\n",
    "    - Create model with few-shot prompt\n",
    "    - Evaluate\n",
    "    - Try with few-shot examples\n",
    "    - Evaluate\n",
    "    - Compare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc19dd8-c467-4dc0-a2b1-16346ed4864d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b252228-7ea6-4c44-98a6-534c7346e107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LANGCHAIN_API_KEY=ls__ecf0f921dd3e42bf83e7a6e356d5978b\n"
     ]
    }
   ],
   "source": [
    "%env LANGCHAIN_API_KEY=ls__ecf0f921dd3e42bf83e7a6e356d5978b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80990f3-10e3-4a40-91d7-99b9dd52d8ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c06ac76-71cb-453f-af20-75926e05edbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for split in ['train', 'validation', 'test']:\n",
    "    name = f\"Rebel-linearized-{split}\"\n",
    "#     client.upload_csv(\n",
    "#         f\"data/{name}.csv\",\n",
    "#           input_keys=[\"context\"],\n",
    "#           output_keys=[\"triplets\"],\n",
    "#           name=name + \"foo\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a699d7f1-3b29-411e-84ce-bec19de4957e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'The feature appears in U . S . Navy aerial photographs taken in the 1960s and in imagery obtained by the NASA Earth Resources Technology Satellite ( ERTS-1 ) , 1973â€“74 . '}\n",
      "{'triplets': '<triplet> Earth Resources Technology Satellite <subj> NASA <obj> operator'}\n"
     ]
    }
   ],
   "source": [
    "example_format = next(client.list_examples(dataset_name=\"Rebel-linearized-train\"))\n",
    "print(example_format.inputs)\n",
    "print(example_format.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af16e4e-c357-4429-becc-31b023f8b972",
   "metadata": {},
   "source": [
    "## Define an evaluator\n",
    "\n",
    "We will use a custom LLM evaluator here because we are lazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc458876-a557-4cd3-ba3f-dd5e18c93be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "from langchain.evaluation import StringEvaluator\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import openai_functions\n",
    "import json\n",
    "\n",
    "eval_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an impartial grader tasked with measuring the accuracy of extracted entity relations.\"),\n",
    "        (\"human\", \"Please evaluate the following data:\\n\\n\"\n",
    "         \"<INPUT>\\n{input}</INPUT>\\n\"\n",
    "         \"<PREDICTED>\\n{prediction}</PREDICTED>\\n\"\n",
    "         \"<GROUND_TRUTH>\\n{reference}</GROUND_TRUTH>\\n\\n\"\n",
    "         \"Please save your reasoning and grading by calling the commit_grade function.\"\n",
    "         \" First, enumerate all factual discrepancies in the predicted triplets relative to the ground truth.\"\n",
    "         \" Finally, score the prediction on a scale out of 100, taking into account factuality and\"\n",
    "         \" correctness according to the ground truth.\"),\n",
    "         \n",
    "    ]\n",
    ")\n",
    "\n",
    "commit_grade_schema = {\n",
    "    \"name\": \"commit_grade\",\n",
    "    \"description\": \"Commits a grade with reasoning.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"commit_grade_parameters\",\n",
    "        \"description\": \"Parameters for the commit_grade function.\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"mistakes\": {\n",
    "                \"title\": \"discrepancies\",\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any discrepencies between the predicted and ground truth.\"\n",
    "            },\n",
    "            \"reasoning\": {\n",
    "                \"title\": \"reasoning\",\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The explanation or logic behind the final grade.\"\n",
    "            },\n",
    "            \"grade\": {\n",
    "                \"title\": \"grade\",\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The numerical value representing the grade.\",\n",
    "                \"minimum\": 0,\n",
    "                \"maximum\": 100\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"reasoning\", \"grade\", \"mistakes\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_grade(func_args: str) -> dict:\n",
    "    args = json.loads(func_args)\n",
    "    return {\n",
    "        \"reasoning\": (args.get(\"reasoning\", \"\") + \"\\n\\n\" + args.get(\"discrepancies\", \"\")).strip(),\n",
    "        \"score\": args.get(\"grade\", 0) / 100,\n",
    "    }\n",
    "\n",
    "eval_chain = (\n",
    "    eval_prompt \n",
    "    | ChatOpenAI(model=\"gpt-4\", temperature=0).bind(functions=[commit_grade_schema])\n",
    "    | openai_functions.OutputFunctionsParser() \n",
    "    | normalize_grade\n",
    "                                     \n",
    ")\n",
    "\n",
    "class EvaluateTriplets(StringEvaluator):\n",
    "    \"\"\"Evaluate the triplets of a predicted string.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def requires_input(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    @property\n",
    "    def requires_reference(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _evaluate_strings(\n",
    "        self,\n",
    "        *,\n",
    "        prediction: str,\n",
    "        reference: Optional[str] = None,\n",
    "        input: Optional[str] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> dict:\n",
    "        callbacks = kwargs.pop(\"callbacks\", None)\n",
    "        return eval_chain.invoke(\n",
    "            {\"prediction\": prediction, \"reference\": reference, \"input\": input}, \n",
    "            {\"callbacks\": callbacks},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd17ef6-62a5-46f7-b591-4b507070ff23",
   "metadata": {},
   "source": [
    "#### Define Baseline Chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8228fbb-775a-491e-bfa9-1b8807720e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ebb788-4506-4496-b0d8-458f2266d11b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will focus on an instructional prompt based on the format\n",
    "# description of the dataset.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an autoregressive information extraction agent.\"),\n",
    "        (\"human\", \"Extract knowledge triplets from the following text:\\n\"\n",
    "        \"<TEXT>\\n{context}\\n<\\/TEXT>\"),\n",
    "        (\"system\", \"Output should be in linearized format.<triplet> marks the start of a new triplet with\"\n",
    "        \"a new head entity, followed by the surface form\"\n",
    "        \"of that entity in the input text. <subj> marks\"\n",
    "        \"the end of the head entity and the start of the tail\"\n",
    "        \"entity surface form. <obj> marks the end of the\"\n",
    "        \"tail entity and the start of the relation between the\"\n",
    "        \"head and tail entity, in its surface form\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48b9ddda-0e5b-4239-8144-c04641469a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_dataset_name = \"Rebel-linearized-validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df551c3e-d808-4fe3-8cd3-6a3dafed190d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "config = RunEvalConfig(\n",
    "    custom_evaluators=[EvaluateTriplets()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dece101b-2f8b-4d10-b35b-653a094d4f92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'e9369e73b2e94d218aecf06a51af866a-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/a0d94242-89d1-42af-b851-4fa6e8a3795b?eval=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} <CIMultiDictProxy('Date': 'Fri, 18 Aug 2023 21:45:08 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7f8d48edbd8dceb5-SJC', 'alt-svc': 'h3=\":443\"; ma=86400')>.\n"
     ]
    }
   ],
   "source": [
    "_ = await client.arun_on_dataset(validation_dataset_name, chain, evaluation=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10683ab1-aa5a-4cee-95c4-60830786ea9a",
   "metadata": {},
   "source": [
    "## Now let's try it with some few-shot examples\n",
    "\n",
    "We'll first try with some static examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e69a4ef2-b52c-4358-9f56-cee48d8ae6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "def create_few_shot_prompt(examples):\n",
    "    formatted_examples = [{**ex.inputs, **ex.outputs} for ex in examples]\n",
    "    # This is a prompt template used to format each individual example.\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"Extract knowledge triplets from the following text:\"\n",
    "            \"\\n<TEXT>\\n{context}</TEXT>\"),\n",
    "            (\"ai\", \"{triplets}\"),\n",
    "        ]\n",
    "    )\n",
    "    return FewShotChatMessagePromptTemplate(\n",
    "        example_prompt=example_prompt,\n",
    "        examples=formatted_examples,\n",
    "    )\n",
    "\n",
    "def create_chain_from_examples(examples):\n",
    "    few_shot_prompt = create_few_shot_prompt(examples)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an autoregressive information extraction agent.\"),\n",
    "            few_shot_prompt,\n",
    "            (\"human\", \"Extract knowledge triplets from the following text:\\n\"\n",
    "            \"<TEXT>\\n{context}\\n<\\/TEXT>\"),\n",
    "            (\"system\", \"Output should be in linearized format.<triplet> marks the start of a new triplet with\"\n",
    "        \"a new head entity, followed by the surface form\"\n",
    "        \"of that entity in the input text. <subj> marks\"\n",
    "        \"the end of the head entity and the start of the tail\"\n",
    "        \"entity surface form. <obj> marks the end of the\"\n",
    "        \"tail entity and the start of the relation between the\"\n",
    "        \"head and tail entity, in its surface form\"),\n",
    "        ]\n",
    "    )\n",
    "    return prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "402eff8c-38a2-4fc8-8b41-4b9a74795868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "train_dataset_name = \"Rebel-linearized-train\"\n",
    "\n",
    "K = 5\n",
    "\n",
    "# Pick the first K rows as few-shot examples\n",
    "examples_head = islice(client.list_examples(dataset_name=train_dataset_name), K)\n",
    "chain_2 = create_chain_from_examples(examples_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0c86aa0-8444-4e90-9777-8ac16f3c83f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'f091482618c54db4a0726b98067e9661-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/441695f7-5a8a-4011-b4ec-297a6e6fd13a?eval=true\n"
     ]
    }
   ],
   "source": [
    "chain_2_results = await client.arun_on_dataset(validation_dataset_name, chain_2, evaluation=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31530d-a6b6-48ba-a347-7e8211cc1eb7",
   "metadata": {},
   "source": [
    "That increases the score by a bit. Let's try to improve it even more by selecting examples\n",
    "that the model \"fails\" on. Presumably, these are \"harder\" and will provide more information\n",
    "for the model to learn from. We don't want to pollute the training set, so we will score on the train set then use the worst examples as few-shot examples. This tactic assumes the labels are indeed high quality, which may not always be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0149bda2-ef76-4ff2-a3ac-d0a96ebc9acc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '10a8199763ed4212ad05376cb90a6373-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/6a28a8bf-733f-43ab-b400-5194a87a9722?eval=true\n"
     ]
    }
   ],
   "source": [
    "# We will run this only on the training set just to score the outputs\n",
    "training_results = await client.arun_on_dataset(train_dataset_name, chain_2, evaluation=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77ecc1c0-3afa-4aeb-9a67-704b6284502b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92e3e854-8992-48b5-931c-7bc09ec5fd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import langsmith # the module you want to reload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41408f3d-c6e5-4c4f-9b4c-2d092af14bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = importlib.reload(langsmith).Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c445ef91-d18a-45c0-aaf9-109dc0c1b3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs = list(client.list_runs(\n",
    "    filter='and(eq(feedback_key, \"EvaluateTriplets\"), lt(feedback_score, 0.1))',\n",
    "    project_name=training_results[\"project_name\"]\n",
    "))\n",
    "example_ids = {r.reference_example_id for r in runs}\n",
    "# examples = [client.list_examples(example_ids=[r.reference_example_id for r in runs])]\n",
    "examples = [e for e in client.list_examples(dataset_id=client.read_example(example_id=runs[0].reference_example_id).dataset_id)\n",
    " if e.id in example_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd9c1383-a464-439b-a3ad-0a8a5397ca97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_3 = create_chain_from_examples(examples[:K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f150de81-55cb-49fb-9b26-3427821ec3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'a37aafc4a00a49c0bbf3386a2834c029-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/5c121572-a15b-48b0-8ae4-9b637fb9bd66?eval=true\n"
     ]
    }
   ],
   "source": [
    "results = await client.arun_on_dataset(validation_dataset_name, chain_3, evaluation=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c9ae5-db22-4d81-8095-c348b07e4e75",
   "metadata": {},
   "source": [
    "This actually **decreased** the score since the examples either are out of distribution or are poorly labeled.\n",
    "\n",
    "Let's try the other way around: select the examples that the original model \"succeeds\" on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5ceb4480-c9a1-46ca-9527-fe49f8b0fbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs = list(client.list_runs(\n",
    "    filter='and(eq(feedback_key, \"EvaluateTriplets\"), gte(feedback_score, 0.7))',\n",
    "    project_name=training_results[\"project_name\"]\n",
    "))\n",
    "example_ids = {r.reference_example_id for r in runs}\n",
    "# examples = [client.list_examples(example_ids=[r.reference_example_id for r in runs])]\n",
    "examples = [e for e in client.list_examples(dataset_id=client.read_example(example_id=runs[0].reference_example_id).dataset_id)\n",
    " if e.id in example_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e4384acd-eae5-4011-ba1d-eabb9918fefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_4 = create_chain_from_examples(examples[:K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "76723200-e4e0-4100-a641-ca3ab4b86f79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '567387171ca943e2bce7d6f08889e768-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/d482c156-2d80-4baf-b37c-fdcdefa98755?eval=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    }
   ],
   "source": [
    "results = await client.arun_on_dataset(validation_dataset_name, chain_3, evaluation=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399002a-0efe-4f58-9458-bc46b27c0428",
   "metadata": {},
   "source": [
    "EVEN WORSE! This validates the common finding that naievely drawing from the ends of the performance distribution\n",
    "is (often) a bad idea. What if we draw from the middle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ba36eb01-8c5c-4f84-bfa9-d88d9b09b56a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs = list(client.list_runs(\n",
    "    filter='and(eq(feedback_key, \"EvaluateTriplets\"), lt(feedback_score, 0.55), gte(feedback_score, 0.45))',\n",
    "    project_name=training_results[\"project_name\"]\n",
    "))\n",
    "example_ids = {r.reference_example_id for r in runs}\n",
    "# examples = [client.list_examples(example_ids=[r.reference_example_id for r in runs])]\n",
    "examples = [e for e in client.list_examples(dataset_id=client.read_example(example_id=runs[0].reference_example_id).dataset_id)\n",
    " if e.id in example_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "302d03b1-f396-4539-8a1a-0c6e9c98d70f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_5 = create_chain_from_examples(examples[:K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d3724ec3-d760-41e2-a510-40dd426fb336",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '7d65436001db415894c636145238da91-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/c455467e-e2e3-4bb6-b03f-410776195a60?eval=true\n"
     ]
    }
   ],
   "source": [
    "results = await client.arun_on_dataset(validation_dataset_name, chain_3, evaluation=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6d1e35-3263-4a45-868c-0491732d05d2",
   "metadata": {},
   "source": [
    "This also gives worse results lol. wow what a world we live in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f495ee6-40d9-4809-b8e4-62dfedc3af60",
   "metadata": {},
   "source": [
    "## What if we want to do some meta-prompting to improve the few-shot examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b72ec-0111-43af-b441-e09fb6849496",
   "metadata": {},
   "outputs": [],
   "source": [
    "Try "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
